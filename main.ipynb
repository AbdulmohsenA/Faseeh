{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install wandb evaluate huggingface_hub datasets bert_score evaluate","metadata":{"execution":{"iopub.status.busy":"2024-07-19T13:03:21.687393Z","iopub.execute_input":"2024-07-19T13:03:21.687670Z","iopub.status.idle":"2024-07-19T13:03:36.362734Z","shell.execute_reply.started":"2024-07-19T13:03:21.687646Z","shell.execute_reply":"2024-07-19T13:03:36.361792Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: wandb in /opt/conda/lib/python3.10/site-packages (0.17.4)\nCollecting evaluate\n  Downloading evaluate-0.4.2-py3-none-any.whl.metadata (9.3 kB)\nRequirement already satisfied: huggingface_hub in /opt/conda/lib/python3.10/site-packages (0.23.4)\nRequirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (2.20.0)\nCollecting bert_score\n  Downloading bert_score-0.3.13-py3-none-any.whl.metadata (15 kB)\nRequirement already satisfied: click!=8.0.0,>=7.1 in /opt/conda/lib/python3.10/site-packages (from wandb) (8.1.7)\nRequirement already satisfied: docker-pycreds>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (0.4.0)\nRequirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (3.1.41)\nRequirement already satisfied: platformdirs in /opt/conda/lib/python3.10/site-packages (from wandb) (3.11.0)\nRequirement already satisfied: protobuf!=4.21.0,<6,>=3.19.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (3.20.3)\nRequirement already satisfied: psutil>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from wandb) (6.0.1)\nRequirement already satisfied: requests<3,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (2.32.3)\nRequirement already satisfied: sentry-sdk>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (2.8.0)\nRequirement already satisfied: setproctitle in /opt/conda/lib/python3.10/site-packages (from wandb) (1.3.3)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from wandb) (69.0.3)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from evaluate) (1.26.4)\nRequirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.2.2)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from evaluate) (4.66.4)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from evaluate) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.70.16)\nRequirement already satisfied: fsspec>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.5.0)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from evaluate) (21.3)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (3.13.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (4.9.0)\nRequirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (16.1.0)\nRequirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets) (0.6)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.1)\nRequirement already satisfied: torch>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from bert_score) (2.1.2)\nRequirement already satisfied: transformers>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from bert_score) (4.42.3)\nRequirement already satisfied: matplotlib in /opt/conda/lib/python3.10/site-packages (from bert_score) (3.7.5)\nRequirement already satisfied: six>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.11)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->evaluate) (3.1.1)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2023.4)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2024.7.4)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->bert_score) (1.13.0)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->bert_score) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->bert_score) (3.1.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers>=3.0.0->bert_score) (2023.12.25)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers>=3.0.0->bert_score) (0.4.3)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers>=3.0.0->bert_score) (0.19.1)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->bert_score) (1.2.0)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib->bert_score) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->bert_score) (4.47.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->bert_score) (1.4.5)\nRequirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->bert_score) (9.5.0)\nRequirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.0.0->bert_score) (2.1.3)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.0.0->bert_score) (1.3.0)\nDownloading evaluate-0.4.2-py3-none-any.whl (84 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading bert_score-0.3.13-py3-none-any.whl (61 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: evaluate, bert_score\nSuccessfully installed bert_score-0.3.13 evaluate-0.4.2\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport re\nimport torch\nimport os\nfrom torch.utils.data import DataLoader, random_split, Dataset\nimport requests\nimport ray\nfrom datasets import load_dataset\n\n# USE RAY TUNE. https://docs.ray.io/en/latest/train/examples/intel_gaudi/bert.html\n# USE E5SCORE AS A LOSS\n# deepl translations","metadata":{"execution":{"iopub.status.busy":"2024-07-19T13:10:38.912355Z","iopub.execute_input":"2024-07-19T13:10:38.913130Z","iopub.status.idle":"2024-07-19T13:10:38.918655Z","shell.execute_reply.started":"2024-07-19T13:10:38.913097Z","shell.execute_reply":"2024-07-19T13:10:38.917695Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"import wandb\nfrom huggingface_hub import HfApi, HfFolder\nfrom transformers import set_seed\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\n\nHF_TOKEN = user_secrets.get_secret(\"HF_TOKEN\")\nWANDB_KEY = user_secrets.get_secret(\"WANDB_KEY\")\n\nHfFolder.save_token(HF_TOKEN)\nwandb.login(key=WANDB_KEY)\n\nseed = 1\ntorch.cuda.manual_seed_all(seed)\ntorch.backends.cudnn.deterministic = True # Should be True, but False to save memory\ntorch.backends.cudnn.benchmark = False # Should be False, but True to save memory\nset_seed(seed)\nnp.random.seed(seed)","metadata":{"execution":{"iopub.status.busy":"2024-07-19T13:10:47.096723Z","iopub.execute_input":"2024-07-19T13:10:47.097269Z","iopub.status.idle":"2024-07-19T13:10:47.565382Z","shell.execute_reply.started":"2024-07-19T13:10:47.097240Z","shell.execute_reply":"2024-07-19T13:10:47.564486Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mabdulmohsena\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Quran Dataset","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv(\"hf://datasets/ImruQays/Quran-Classical-Arabic-English-Parallel-texts/Quran-translations.csv\")","metadata":{"execution":{"iopub.status.busy":"2024-07-19T13:10:48.808957Z","iopub.execute_input":"2024-07-19T13:10:48.809561Z","iopub.status.idle":"2024-07-19T13:10:50.236977Z","shell.execute_reply.started":"2024-07-19T13:10:48.809530Z","shell.execute_reply":"2024-07-19T13:10:50.235977Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"data = df.copy()\n\ndata = data.drop(columns=[data.columns[0]])","metadata":{"execution":{"iopub.status.busy":"2024-07-19T13:10:50.238747Z","iopub.execute_input":"2024-07-19T13:10:50.239037Z","iopub.status.idle":"2024-07-19T13:10:50.256604Z","shell.execute_reply.started":"2024-07-19T13:10:50.239013Z","shell.execute_reply":"2024-07-19T13:10:50.255423Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# Connect Contextual verses\nconnection_signs = ['-', '—', ':', ';', ',']\n\nfor index, row in data.iterrows():\n    \n    total_connected_sentences = sum(row[column].endswith(connection_sign) \n                                    for column in data.columns for connection_sign in connection_signs)\n\n    # If there is at least 3 translations that say the Ayah is connected with the next, then connect them.\n    if total_connected_sentences >= 3:\n        for column in data.columns:\n            data.at[index + 1, column] = f\"{data.at[index, column]} {data.at[index + 1, column]}\"\n            data.at[index, column] = np.nan\n\ndata.dropna(inplace=True)\ndata.reset_index(drop=True, inplace=True)\n\ndisplay(data.info())","metadata":{"execution":{"iopub.status.busy":"2024-07-19T13:10:50.258019Z","iopub.execute_input":"2024-07-19T13:10:50.258875Z","iopub.status.idle":"2024-07-19T13:10:55.609317Z","shell.execute_reply.started":"2024-07-19T13:10:50.258842Z","shell.execute_reply":"2024-07-19T13:10:55.608372Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 4598 entries, 0 to 4597\nData columns (total 18 columns):\n #   Column          Non-Null Count  Dtype \n---  ------          --------------  ----- \n 0   arabic-imlaei   4598 non-null   object\n 1   en-ahmedali     4598 non-null   object\n 2   en-ahmedraza    4598 non-null   object\n 3   en-arberry      4598 non-null   object\n 4   en-asad         4598 non-null   object\n 5   en-daryabadi    4598 non-null   object\n 6   en-hilali       4598 non-null   object\n 7   en-itani        4598 non-null   object\n 8   en-maududi      4598 non-null   object\n 9   en-mubarakpuri  4598 non-null   object\n 10  en-pickthall    4598 non-null   object\n 11  en-qarai        4598 non-null   object\n 12  en-qaribullah   4598 non-null   object\n 13  en-sahih        4598 non-null   object\n 14  en-sarwar       4598 non-null   object\n 15  en-shakir       4598 non-null   object\n 16  en-wahiduddi    4598 non-null   object\n 17  en-yusufali     4598 non-null   object\ndtypes: object(18)\nmemory usage: 646.7+ KB\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"None"},"metadata":{}}]},{"cell_type":"code","source":"def prepare_text(text):\n    # Convert text to lowercase\n    text = text.lower()\n    \n    # Remove text between parentheses/brakets (Explanatory text which is not originally written in arabic)\n    text = re.sub(r'\\([^)]*\\)|\\[[^]]*\\]', '', text)\n    \n    # Remove dashes and commas\n    text = re.sub(r'[-,:;’‘\\\"\\']+', '', text)\n    \n    # Remove extra spaces\n    text = re.sub(r'\\s+', ' ', text)\n    \n    arabic_diacritics = re.compile(\"\"\"\n        ّ    | # Shadda\n        َ    | # Fatha\n        ً    | # Tanwin Fath\n        ُ    | # Damma\n        ٌ    | # Tanwin Damm\n        ِ    | # Kasra\n        ٍ    | # Tanwin Kasr\n        ْ    | # Sukun\n        ـ    # Tatweel (kashida)\n    \"\"\", re.VERBOSE)\n    \n    text = re.sub(arabic_diacritics, '', text)\n    \n    return text\n\ndata = data.map(prepare_text)","metadata":{"execution":{"iopub.status.busy":"2024-07-19T13:10:55.611817Z","iopub.execute_input":"2024-07-19T13:10:55.612498Z","iopub.status.idle":"2024-07-19T13:10:58.885094Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Remove first Ayat (Usually \"Bismillah\" is not translated)\ndata = data[~data.iloc[:, 0].str.contains('بسم الله', na=False)]\ndata.reset_index(inplace=True, drop=True)","metadata":{"execution":{"iopub.status.busy":"2024-07-19T13:10:58.901590Z","iopub.execute_input":"2024-07-19T13:10:58.901866Z","iopub.status.idle":"2024-07-19T13:10:58.914985Z","shell.execute_reply.started":"2024-07-19T13:10:58.901844Z","shell.execute_reply":"2024-07-19T13:10:58.914143Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# Inspecting the translation quality\nn = 123\n\nprint(data.iloc[n, 0], \"\\n\")\nprint(\"\\n\".join(row for row in data.iloc[n, 1:].tolist()))","metadata":{"execution":{"iopub.status.busy":"2024-07-19T13:11:12.655775Z","iopub.execute_input":"2024-07-19T13:11:12.656135Z","iopub.status.idle":"2024-07-19T13:11:12.662279Z","shell.execute_reply.started":"2024-07-19T13:11:12.656105Z","shell.execute_reply":"2024-07-19T13:11:12.661247Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"قولوا آمنا بالله وما أنزل إلينا وما أنزل إلىٰ إبراهيم وإسماعيل وإسحاق ويعقوب والأسباط وما أوتي موسىٰ وعيسىٰ وما أوتي النبيون من ربهم لا نفرق بين أحد منهم ونحن له مسلمون \n\nsay we believe in god and what has been sent down to us and what had been revealed to abraham and ishmael and isaac and jacob and their progeny and that which was given to moses and christ and to all other prophets by the lord. we make no distinction among them and we submit to him.\nsay “we believe in allah and what is sent down to us and what was sent down to ibrahim and ismael and ishaq and yaqub and to their offspring and what was bestowed upon moosa and eisa and what was bestowed upon other prophets – from their lord we do not make any distinction in belief between any of them and to allah we have submitted ourselves.”\nsay you we believe in god and in that which has been sent down on us and sent down on abraham ishmael isaac and jacob and the tribes and that which was given to moses and jesus and the prophets of their lord we make no division between any of them and to him we surrender.\nsay we believe in god and in that which has been bestowed from on high upon us and that which has been bestowed upon abraham and ishmael and isaac and jacob and their descendants and that which has been vouchsafed to moses and jesus and that which has been vouchsafed to all the prophets by their sustainer we make no distinction between any of them. and it is unto him that we surrender ourselves.\nsay we believe in allah and that which hath been sent down unto us and that which was sent down unto ibrahim and ismail and ishaq and yaqub and the tribes and that which was vouchsafed unto musa and lsa and that which was vouchsafed unto the prophets from their lord we differentiate not between any of them and unto him are submissive.\nsay we believe in allah and that which has been sent down to us and that which has been sent down to ibrahim ismail ishaque yaqub and to alasbat and that which has been given to musa and iesa and that which has been given to the prophets from their lord. we make no distinction between any of them and to him we have submitted .\nsay “we believe in god and in what was revealed to us and in what was revealed to abraham and ishmael and isaac and jacob and the patriarchs and in what was given to moses and jesus and in what was given to the prophets—from their lord. we make no distinction between any of them and to him we surrender.”\no muslims say to them we believe in allah and the guidance which has been sent down to us and which was sent to abraham ismail isaac and jacob and his descendants and which was given by their lord to moses and jesus and to all other prophets. we do not discriminate against any of them and we have completely surrendered to allah as muslims.\nsay we believe in allah and that which has been sent down to us and that which has been sent down to ibrahim isma`il ishaq ya`qub and to alasbat and that which has been given to musa and `isa and that which has been given to the prophets from their lord. we make no distinction between any of them and to him we have submitted .\nsay we believe in allah and that which is revealed unto us and that which was revealed unto abraham and ishmael and isaac and jacob and the tribes and that which moses and jesus received and that which the prophets received from their lord. we make no distinction between any of them and unto him we have surrendered.\nsay we have faith in allah and what has been sent down to us and what was sent down to abraham ishmael isaac jacob and the tribes and that which moses and jesus were given and that which the prophets were given from their lord we make no distinction between any of them and to him do we submit.\nsay we believe in allah and that which is sent down to us and in what was sent down to abraham ishmael isaac jacob and the tribes to moses and jesus and the prophets from their lord. we do not differentiate between any of them and to him we are submissive \nsay we have believed in allah and what has been revealed to us and what has been revealed to abraham and ishmael and isaac and jacob and the descendants and what was given to moses and jesus and what was given to the prophets from their lord. we make no distinction between any of them and we are muslims to him.\n say we believe in god and what he has revealed to us and to abraham ishmael isaac and their descendants and what was revealed to moses jesus and the prophets from their lord. we make no distinction among them and to god we have submitted ourselves.\nsay we believe in allah and that which had been revealed to us and that which was revealed to ibrahim and ismail and ishaq and yaqoub and the tribes and that which was given to musa and isa and that which was given to the prophets from their lord we do not make any distinction between any of them and to him do we submit.\nsay we believe in god and what was revealed to us and what was revealed to abraham ishmael isaac jacob and their descendants and what was given to moses and jesus and what was given to the prophets by their lord. we make no distinction between any of them. it is to him that we surrender ourselves.\nsay ye we believe in allah and the revelation given to us and to abraham ismail isaac jacob and the tribes and that given to moses and jesus and that given to prophets from their lord we make no difference between one and another of them and we bow to allah .\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Preparation","metadata":{}},{"cell_type":"code","source":"data = data.copy()\n\ny = data.iloc[:, 0]\nX = data.iloc[:, 1:]","metadata":{"execution":{"iopub.status.busy":"2024-07-19T13:11:17.240146Z","iopub.execute_input":"2024-07-19T13:11:17.240771Z","iopub.status.idle":"2024-07-19T13:11:17.248191Z","shell.execute_reply.started":"2024-07-19T13:11:17.240739Z","shell.execute_reply":"2024-07-19T13:11:17.247236Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"pairs_list = []\n\nfor n_row in range(X.shape[0]):\n    for n_col in range(X.shape[1]):\n        \n        pairs_list.append([X.iloc[n_row, n_col], y[n_row]])\n        \npairs_list = np.array(pairs_list)","metadata":{"execution":{"iopub.status.busy":"2024-07-19T13:11:18.899251Z","iopub.execute_input":"2024-07-19T13:11:18.899615Z","iopub.status.idle":"2024-07-19T13:11:22.644524Z","shell.execute_reply.started":"2024-07-19T13:11:18.899587Z","shell.execute_reply":"2024-07-19T13:11:22.643601Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"pairs_dict = {\n    \"source\" : pairs_list[:, 0],\n    \"target\" : pairs_list[:, 1]\n} ","metadata":{"execution":{"iopub.status.busy":"2024-07-19T13:12:11.481388Z","iopub.execute_input":"2024-07-19T13:12:11.481785Z","iopub.status.idle":"2024-07-19T13:12:11.487647Z","shell.execute_reply.started":"2024-07-19T13:12:11.481757Z","shell.execute_reply":"2024-07-19T13:12:11.485875Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"from datasets import Dataset\n\nquran_ds = Dataset.from_dict(pairs_dict)\nquran_ds = quran_ds.shuffle(seed=seed)","metadata":{"execution":{"iopub.status.busy":"2024-07-19T13:21:22.404559Z","iopub.execute_input":"2024-07-19T13:21:22.405515Z","iopub.status.idle":"2024-07-19T13:21:25.996227Z","shell.execute_reply.started":"2024-07-19T13:21:22.405479Z","shell.execute_reply":"2024-07-19T13:21:25.995268Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"quran_ds[1]","metadata":{"execution":{"iopub.status.busy":"2024-07-19T13:21:27.640424Z","iopub.execute_input":"2024-07-19T13:21:27.641059Z","iopub.status.idle":"2024-07-19T13:21:27.647142Z","shell.execute_reply.started":"2024-07-19T13:21:27.641028Z","shell.execute_reply":"2024-07-19T13:21:27.646156Z"},"trusted":true},"execution_count":42,"outputs":[{"execution_count":42,"output_type":"execute_result","data":{"text/plain":"{'source': 'say obey allah and obey the messenger but if you turn away he is only responsible for the duty placed on him and you for that placed on you. if you obey him you shall be on the right guidance. the messengers duty is only to convey in a clear way .',\n 'target': 'قل أطيعوا الله وأطيعوا الرسول فإن تولوا فإنما عليه ما حمل وعليكم ما حملتم وإن تطيعوه تهتدوا وما على الرسول إلا البلاغ المبين'}"},"metadata":{}}]},{"cell_type":"markdown","source":"## Shamela Books","metadata":{}},{"cell_type":"code","source":"# Shamela URL is a drive csv file. Arabic Text is scraped first from https://shamela.ws/ and then batch translated \\\n# in google translate (Files). Then prepared as a dataset of sources & targets.\n\nshamela_url = user_secrets.get_secret(\"SHAMELA_DS\")\n\nshamela_df = pd.read_csv(shamela_url)","metadata":{"execution":{"iopub.status.busy":"2024-07-19T13:19:56.612790Z","iopub.execute_input":"2024-07-19T13:19:56.613447Z","iopub.status.idle":"2024-07-19T13:20:00.562653Z","shell.execute_reply.started":"2024-07-19T13:19:56.613413Z","shell.execute_reply":"2024-07-19T13:20:00.561642Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"# Remove index col\nshamela_df = shamela_df.iloc[:, 1:]\n\n# Prepare text\nshamela_df = shamela_df.map(prepare_text)","metadata":{"execution":{"iopub.status.busy":"2024-07-19T13:20:01.801809Z","iopub.execute_input":"2024-07-19T13:20:01.802412Z","iopub.status.idle":"2024-07-19T13:20:02.589751Z","shell.execute_reply.started":"2024-07-19T13:20:01.802378Z","shell.execute_reply":"2024-07-19T13:20:02.588988Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"shamela_ds = Dataset.from_dict({\n    \"source\" : shamela_df.iloc[:, 1],\n    \"target\" : shamela_df.iloc[:, 0]\n})\n\nshamela_ds = shamela_ds.shuffle(seed=seed)","metadata":{"execution":{"iopub.status.busy":"2024-07-19T13:20:08.370607Z","iopub.execute_input":"2024-07-19T13:20:08.370964Z","iopub.status.idle":"2024-07-19T13:20:08.423875Z","shell.execute_reply.started":"2024-07-19T13:20:08.370936Z","shell.execute_reply":"2024-07-19T13:20:08.422899Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"shamela_ds[1]","metadata":{"execution":{"iopub.status.busy":"2024-07-19T13:20:10.557916Z","iopub.execute_input":"2024-07-19T13:20:10.558782Z","iopub.status.idle":"2024-07-19T13:20:10.564914Z","shell.execute_reply.started":"2024-07-19T13:20:10.558749Z","shell.execute_reply":"2024-07-19T13:20:10.563994Z"},"trusted":true},"execution_count":40,"outputs":[{"execution_count":40,"output_type":"execute_result","data":{"text/plain":"{'source': ' he was not satisfied with me bringing this to talk until he had me as a martyr and he was not satisfied with my martyrdom until he made me swear an oath.',\n 'target': 'فلم يرض بإحضاري هذا لكلام حتى استشهدني ولم يرض باستشهادي حتى استحلفني.'}"},"metadata":{}}]},{"cell_type":"markdown","source":"## Combining Datasets","metadata":{}},{"cell_type":"code","source":"used_datasets = [quran_ds.to_pandas(), shamela_ds.to_pandas()]\n\ndataset = Dataset.from_pandas(pd.concat(used_datasets))\ndataset = dataset.shuffle(seed=seed)\n\n# Remove very short sentences\ndataset = dataset.filter(lambda x: len(x['source']) > 10)","metadata":{"execution":{"iopub.status.busy":"2024-07-19T13:28:31.076051Z","iopub.execute_input":"2024-07-19T13:28:31.076421Z","iopub.status.idle":"2024-07-19T13:28:33.476089Z","shell.execute_reply.started":"2024-07-19T13:28:31.076390Z","shell.execute_reply":"2024-07-19T13:28:33.475180Z"},"trusted":true},"execution_count":67,"outputs":[{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/83369 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b1aaeb269b234ade827c1aec63053729"}},"metadata":{}}]},{"cell_type":"code","source":"dataset[0]","metadata":{"execution":{"iopub.status.busy":"2024-07-19T13:28:33.477635Z","iopub.execute_input":"2024-07-19T13:28:33.477914Z","iopub.status.idle":"2024-07-19T13:28:33.484229Z","shell.execute_reply.started":"2024-07-19T13:28:33.477889Z","shell.execute_reply":"2024-07-19T13:28:33.483317Z"},"trusted":true},"execution_count":68,"outputs":[{"execution_count":68,"output_type":"execute_result","data":{"text/plain":"{'source': 'as for those who disbelieved and gave the lie to our signs they shall be the inmates of the fire and will abide in it. that is a woeful resort!',\n 'target': 'والذين كفروا وكذبوا بآياتنا أولٰئك أصحاب النار خالدين فيها وبئس المصير',\n '__index_level_0__': 15436}"},"metadata":{}}]},{"cell_type":"code","source":"print(\"Percentage of each sub_dataset to the whole dataset:\")\n\n# TODO: Increase the amount of shamela books.\nfor i, ds in enumerate(used_datasets):\n    print(f\"{i}: {len(ds) / sum([len(d) for d in used_datasets]):.2%}\")","metadata":{"execution":{"iopub.status.busy":"2024-07-19T13:32:22.346700Z","iopub.execute_input":"2024-07-19T13:32:22.347031Z","iopub.status.idle":"2024-07-19T13:32:22.352633Z","shell.execute_reply.started":"2024-07-19T13:32:22.347006Z","shell.execute_reply":"2024-07-19T13:32:22.351657Z"},"trusted":true},"execution_count":74,"outputs":[{"name":"stdout","text":"Percentage of each sub_dataset to the whole dataset:\n0: 91.43%\n1: 8.57%\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Modeling","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, GenerationConfig\nfrom transformers import DataCollatorForSeq2Seq","metadata":{"execution":{"iopub.status.busy":"2024-07-19T13:32:58.713156Z","iopub.execute_input":"2024-07-19T13:32:58.713938Z","iopub.status.idle":"2024-07-19T13:33:00.113072Z","shell.execute_reply.started":"2024-07-19T13:32:58.713903Z","shell.execute_reply":"2024-07-19T13:33:00.112278Z"},"trusted":true},"execution_count":75,"outputs":[]},{"cell_type":"code","source":"# # Configure any model from HF HUB\n# assert input(\"YOU WILL REMOVE THE HUB MODEL FOR THIS, TYPE 'OK' TO PROCEED: \").upper() == 'OK'\n# model_name = \"facebook/mbart-large-50-many-to-many-mmt\"\n# model_name = \"facebook/m2m100_1.2B\"\n# #model_name= \"Helsinki-NLP/opus-mt-en-ar\"\n# model_name= \"facebook/nllb-200-distilled-600M\"\n\n# tokenizer = AutoTokenizer.from_pretrained(model_name)\n# model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n# generation_config = GenerationConfig(\n#     max_length=200,\n#     forced_bos_token_id=256011, # Arabic\n#     num_beams = 4,\n#     early_stopping=True,\n#     do_sample=True,\n#     top_k=50,\n    \n#     # Testing Config\n# #     num_return_sequences=4, # Number of sentences to generate\n# #     return_dict_in_generate=True, # Returns the complete generation data from within the model.\n# #     output_scores=True, # Score of each token.\n# )\n\n# tokenizer.src_lang=\"eng_Latn\"\n# tokenizer.tgt_lang=\"arb_Arab\"\n\n# model.push_to_hub(\"Abdulmohsena/Faseeh\")\n# tokenizer.push_to_hub(\"Abdulmohsena/Faseeh\")","metadata":{"execution":{"iopub.status.busy":"2024-07-15T16:27:12.085447Z","iopub.execute_input":"2024-07-15T16:27:12.085792Z","iopub.status.idle":"2024-07-15T16:27:12.090538Z","shell.execute_reply.started":"2024-07-15T16:27:12.085762Z","shell.execute_reply":"2024-07-15T16:27:12.089577Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"# Instantiating The Model\nmodel_name = \"Abdulmohsena/Faseeh\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_name, src_lang=\"eng_Latn\", tgt_lang=\"arb_Arab\")\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_name)\ngeneration_config = GenerationConfig.from_pretrained(model_name)\n\n# https://huggingface.co/docs/transformers/en/main_classes/text_generation","metadata":{"execution":{"iopub.status.busy":"2024-07-19T13:35:16.260373Z","iopub.execute_input":"2024-07-19T13:35:16.261115Z","iopub.status.idle":"2024-07-19T13:36:13.818020Z","shell.execute_reply.started":"2024-07-19T13:35:16.261085Z","shell.execute_reply":"2024-07-19T13:36:13.817097Z"},"trusted":true},"execution_count":76,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/40.1k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"305cf9fd65fe48a98fe4bee64c6e7f0d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentencepiece.bpe.model:   0%|          | 0.00/4.85M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d91ebf42f4ba4f9ea9778f9ae2a7f789"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/17.3M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"85e4efeae4724fa39531c66343fcd934"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/4.23k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0505357a65f947f5baa46fa8804b6526"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/883 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a384170336964f458e5852c593304e3c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/2.46G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8b5c7d1aa19042b691f61de6b0d92d80"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/217 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"68d42c1becf74524a1bb40ccd4fe351e"}},"metadata":{}}]},{"cell_type":"code","source":"# Sanity Check\ndummy = \"And the Saudi Arabian Foreign Minister assured the visitors of the importance to seek the security.\"\n\nencoded_ar = tokenizer(dummy, return_tensors=\"pt\")\ngenerated_tokens = model.generate(**encoded_ar, generation_config=generation_config)\n\ntokenizer.decode(generated_tokens[0], skip_special_tokens=True)","metadata":{"execution":{"iopub.status.busy":"2024-07-19T13:37:01.271384Z","iopub.execute_input":"2024-07-19T13:37:01.272040Z","iopub.status.idle":"2024-07-19T13:37:03.237270Z","shell.execute_reply.started":"2024-07-19T13:37:01.272009Z","shell.execute_reply":"2024-07-19T13:37:03.236371Z"},"trusted":true},"execution_count":79,"outputs":[{"execution_count":79,"output_type":"execute_result","data":{"text/plain":"'وآمن وزير خارجية السعودية الزوار بأهمية طلب الأمن.'"},"metadata":{}}]},{"cell_type":"code","source":"def preprocess_function(examples):\n    model_inputs = tokenizer(examples['source'], text_target=examples['target'], max_length=128, truncation=True, padding=True)\n    return model_inputs\n\ntokenized_dataset = dataset.map(preprocess_function, batched=True)\ntokenized_dataset = tokenized_dataset.train_test_split(test_size=0.25)","metadata":{"execution":{"iopub.status.busy":"2024-07-19T13:37:10.632511Z","iopub.execute_input":"2024-07-19T13:37:10.633447Z","iopub.status.idle":"2024-07-19T13:37:30.702252Z","shell.execute_reply.started":"2024-07-19T13:37:10.633412Z","shell.execute_reply":"2024-07-19T13:37:30.701489Z"},"trusted":true},"execution_count":80,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/82903 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7a5dc8ef12a1484e814b0ca54662bc00"}},"metadata":{}}]},{"cell_type":"code","source":"data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model_name)\n\n# from transformers import DataCollatorForLanguageModeling\n\n# data_collator = DataCollatorForLanguageModeling(\n#     tokenizer=tokenizer,\n#     mlm=True,             # Whether to use Masked Language Modeling (MLM)\n#     mlm_probability=0.15  # Probability of masking tokens for MLM\n# )","metadata":{"execution":{"iopub.status.busy":"2024-07-19T13:37:32.738929Z","iopub.execute_input":"2024-07-19T13:37:32.739829Z","iopub.status.idle":"2024-07-19T13:37:32.744046Z","shell.execute_reply.started":"2024-07-19T13:37:32.739795Z","shell.execute_reply":"2024-07-19T13:37:32.743116Z"},"trusted":true},"execution_count":82,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport evaluate\nimport transformers\n\nmetric = evaluate.load(\"bertscore\")\n\ndef postprocess_text(preds, labels):\n    preds = [pred.strip() for pred in preds]\n    labels = [label.strip() for label in labels]\n\n    return preds, labels\n\ndef compute_metrics(eval_preds): \n    \n    preds, labels = eval_preds\n    \n    # Replace unknown labels\n    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n    \n    # Decode tokens into text\n    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n\n    # Postprocess text for cleaniness\n    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n\n    # Get Average bertscore F-1\n    result = metric.compute(predictions=decoded_preds, references=decoded_labels, lang=\"ar\")\n    result = {\"bertscore-f1\": np.mean(result['f1'])}\n\n    # Get avg gen length\n    prediction_lengths = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n    result[\"gen_len\"] = np.mean(prediction_lengths)\n\n    result = {k: round(v, 4) for k, v in result.items()} # Round to 4 figures\n\n    return result","metadata":{"execution":{"iopub.status.busy":"2024-07-19T13:37:50.231077Z","iopub.execute_input":"2024-07-19T13:37:50.231712Z","iopub.status.idle":"2024-07-19T13:37:51.760121Z","shell.execute_reply.started":"2024-07-19T13:37:50.231679Z","shell.execute_reply":"2024-07-19T13:37:51.759178Z"},"trusted":true},"execution_count":83,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/7.95k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"048b356626584d97a1f6a3f8ccbcef88"}},"metadata":{}}]},{"cell_type":"code","source":"from datetime import datetime\nwandb.init(project=\"Faseeh\", name=f\"Run @ {datetime.now()}\")","metadata":{"execution":{"iopub.status.busy":"2024-07-19T13:40:07.135691Z","iopub.execute_input":"2024-07-19T13:40:07.136058Z","iopub.status.idle":"2024-07-19T13:40:23.898901Z","shell.execute_reply.started":"2024-07-19T13:40:07.136030Z","shell.execute_reply":"2024-07-19T13:40:23.898013Z"},"trusted":true},"execution_count":85,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.17.4"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240719_134007-yv6eemhw</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/abdulmohsena/Faseeh/runs/yv6eemhw' target=\"_blank\">Run @ 2024-07-19 13:40:07.137285</a></strong> to <a href='https://wandb.ai/abdulmohsena/Faseeh' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/abdulmohsena/Faseeh' target=\"_blank\">https://wandb.ai/abdulmohsena/Faseeh</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/abdulmohsena/Faseeh/runs/yv6eemhw' target=\"_blank\">https://wandb.ai/abdulmohsena/Faseeh/runs/yv6eemhw</a>"},"metadata":{}},{"execution_count":85,"output_type":"execute_result","data":{"text/html":"<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/abdulmohsena/Faseeh/runs/yv6eemhw?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>","text/plain":"<wandb.sdk.wandb_run.Run at 0x7bc2c6c6e0b0>"},"metadata":{}}]},{"cell_type":"code","source":"from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer\n\ntorch.cuda.empty_cache()\n\n# https://huggingface.co/docs/transformers/v4.42.0/performance\ntraining_args = Seq2SeqTrainingArguments(\n    output_dir=\"Faseeh\",\n    save_total_limit=1,\n    \n    per_device_train_batch_size=4,\n    per_device_eval_batch_size=4,\n    gradient_accumulation_steps=4,\n    gradient_checkpointing=True,\n    # torch_compile=True,\n    \n    logging_strategy=\"steps\",\n    logging_steps=500, \n    \n    eval_strategy='epoch',\n    \n    weight_decay=0.01,\n    warmup_steps=1_000,\n    learning_rate=3e-5,\n    lr_scheduler_type=\"cosine\",\n    \n    \n    num_train_epochs=4,\n    \n    predict_with_generate=True,\n    fp16=True,\n    push_to_hub=True,\n    report_to='wandb'\n)\n\ntrainer = Seq2SeqTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_dataset[\"train\"],\n    eval_dataset=tokenized_dataset[\"test\"],\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n    compute_metrics=compute_metrics,\n)","metadata":{"execution":{"iopub.status.busy":"2024-07-15T17:18:30.493049Z","iopub.execute_input":"2024-07-15T17:18:30.493318Z","iopub.status.idle":"2024-07-15T17:18:31.515562Z","shell.execute_reply.started":"2024-07-15T17:18:30.493295Z","shell.execute_reply":"2024-07-15T17:18:31.514819Z"},"trusted":true},"execution_count":89,"outputs":[]},{"cell_type":"code","source":"trainer.train()\nwandb.finish()","metadata":{"execution":{"iopub.status.busy":"2024-07-15T17:19:43.301899Z","iopub.execute_input":"2024-07-15T17:19:43.302545Z","iopub.status.idle":"2024-07-15T22:08:17.678766Z","shell.execute_reply.started":"2024-07-15T17:19:43.302508Z","shell.execute_reply":"2024-07-15T22:08:17.678014Z"},"trusted":true},"execution_count":90,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='3886' max='3886' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [3886/3886 4:48:26, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Bertscore-f1</th>\n      <th>Gen Len</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>0.015200</td>\n      <td>0.058165</td>\n      <td>0.984500</td>\n      <td>37.320100</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 200}\nYour generation config was originally created from the model config, but the model config has changed since then. Unless you pass the `generation_config` argument to this model's `generate` calls, they will revert to the legacy behavior where the base `generate` parameterization is loaded from the model config instead. To avoid this behavior and this warning, we recommend you to overwrite the generation config model attribute before calling the model's `save_pretrained`, preferably also removing any generation kwargs from the model config. This warning will be raised to an exception in v4.41.\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 200}\nYour generation config was originally created from the model config, but the model config has changed since then. Unless you pass the `generation_config` argument to this model's `generate` calls, they will revert to the legacy behavior where the base `generate` parameterization is loaded from the model config instead. To avoid this behavior and this warning, we recommend you to overwrite the generation config model attribute before calling the model's `save_pretrained`, preferably also removing any generation kwargs from the model config. This warning will be raised to an exception in v4.41.\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 200}\nYour generation config was originally created from the model config, but the model config has changed since then. Unless you pass the `generation_config` argument to this model's `generate` calls, they will revert to the legacy behavior where the base `generate` parameterization is loaded from the model config instead. To avoid this behavior and this warning, we recommend you to overwrite the generation config model attribute before calling the model's `save_pretrained`, preferably also removing any generation kwargs from the model config. This warning will be raised to an exception in v4.41.\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 200}\nYour generation config was originally created from the model config, but the model config has changed since then. Unless you pass the `generation_config` argument to this model's `generate` calls, they will revert to the legacy behavior where the base `generate` parameterization is loaded from the model config instead. To avoid this behavior and this warning, we recommend you to overwrite the generation config model attribute before calling the model's `save_pretrained`, preferably also removing any generation kwargs from the model config. This warning will be raised to an exception in v4.41.\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 200}\nYour generation config was originally created from the model config, but the model config has changed since then. Unless you pass the `generation_config` argument to this model's `generate` calls, they will revert to the legacy behavior where the base `generate` parameterization is loaded from the model config instead. To avoid this behavior and this warning, we recommend you to overwrite the generation config model attribute before calling the model's `save_pretrained`, preferably also removing any generation kwargs from the model config. This warning will be raised to an exception in v4.41.\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 200}\nYour generation config was originally created from the model config, but the model config has changed since then. Unless you pass the `generation_config` argument to this model's `generate` calls, they will revert to the legacy behavior where the base `generate` parameterization is loaded from the model config instead. To avoid this behavior and this warning, we recommend you to overwrite the generation config model attribute before calling the model's `save_pretrained`, preferably also removing any generation kwargs from the model config. This warning will be raised to an exception in v4.41.\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 200}\nYour generation config was originally created from the model config, but the model config has changed since then. Unless you pass the `generation_config` argument to this model's `generate` calls, they will revert to the legacy behavior where the base `generate` parameterization is loaded from the model config instead. To avoid this behavior and this warning, we recommend you to overwrite the generation config model attribute before calling the model's `save_pretrained`, preferably also removing any generation kwargs from the model config. This warning will be raised to an exception in v4.41.\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 200}\nYour generation config was originally created from the model config, but the model config has changed since then. Unless you pass the `generation_config` argument to this model's `generate` calls, they will revert to the legacy behavior where the base `generate` parameterization is loaded from the model config instead. To avoid this behavior and this warning, we recommend you to overwrite the generation config model attribute before calling the model's `save_pretrained`, preferably also removing any generation kwargs from the model config. This warning will be raised to an exception in v4.41.\n/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1364: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='0.004 MB of 0.004 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/bertscore-f1</td><td>▁</td></tr><tr><td>eval/gen_len</td><td>▁</td></tr><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁▂▃▄▅▆▇██</td></tr><tr><td>train/global_step</td><td>▁▂▃▄▅▆▇██</td></tr><tr><td>train/grad_norm</td><td>█▁▃▅▃▃▃</td></tr><tr><td>train/learning_rate</td><td>▄█▇▆▄▂▁</td></tr><tr><td>train/loss</td><td>█▃▁▁▁▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/bertscore-f1</td><td>0.9845</td></tr><tr><td>eval/gen_len</td><td>37.3201</td></tr><tr><td>eval/loss</td><td>0.05817</td></tr><tr><td>eval/runtime</td><td>9230.0527</td></tr><tr><td>eval/samples_per_second</td><td>2.245</td></tr><tr><td>eval/steps_per_second</td><td>0.561</td></tr><tr><td>total_flos</td><td>1.6842736967614464e+16</td></tr><tr><td>train/epoch</td><td>0.99994</td></tr><tr><td>train/global_step</td><td>3886</td></tr><tr><td>train/grad_norm</td><td>0.10105</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.0152</td></tr><tr><td>train_loss</td><td>0.01839</td></tr><tr><td>train_runtime</td><td>17308.9254</td></tr><tr><td>train_samples_per_second</td><td>3.592</td></tr><tr><td>train_steps_per_second</td><td>0.225</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">unique-oath-52</strong> at: <a href='https://wandb.ai/abdulmohsena/Faseeh/runs/yf5k1vbr' target=\"_blank\">https://wandb.ai/abdulmohsena/Faseeh/runs/yf5k1vbr</a><br/> View project at: <a href='https://wandb.ai/abdulmohsena/Faseeh' target=\"_blank\">https://wandb.ai/abdulmohsena/Faseeh</a><br/>Synced 5 W&B file(s), 0 media file(s), 1 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20240715_171809-yf5k1vbr/logs</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."},"metadata":{}}]}]}