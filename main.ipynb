{"cells":[{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-07-19T13:03:21.687670Z","iopub.status.busy":"2024-07-19T13:03:21.687393Z","iopub.status.idle":"2024-07-19T13:03:36.362734Z","shell.execute_reply":"2024-07-19T13:03:36.361792Z","shell.execute_reply.started":"2024-07-19T13:03:21.687646Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: wandb in d:\\anaconda\\envs\\faseeh\\lib\\site-packages (0.17.6)\n","Collecting wandb\n","  Downloading wandb-0.17.7-py3-none-win_amd64.whl.metadata (10 kB)\n","Requirement already satisfied: evaluate in d:\\anaconda\\envs\\faseeh\\lib\\site-packages (0.4.2)\n","Requirement already satisfied: huggingface_hub in d:\\anaconda\\envs\\faseeh\\lib\\site-packages (0.24.5)\n","Requirement already satisfied: datasets in d:\\anaconda\\envs\\faseeh\\lib\\site-packages (2.21.0)\n","Requirement already satisfied: bert_score in d:\\anaconda\\envs\\faseeh\\lib\\site-packages (0.3.13)\n","Requirement already satisfied: transformers in d:\\anaconda\\envs\\faseeh\\lib\\site-packages (4.44.0)\n","Requirement already satisfied: numpy==1.26 in d:\\anaconda\\envs\\faseeh\\lib\\site-packages (1.26.0)\n","Requirement already satisfied: click!=8.0.0,>=7.1 in d:\\anaconda\\envs\\faseeh\\lib\\site-packages (from wandb) (8.1.7)\n","Requirement already satisfied: docker-pycreds>=0.4.0 in d:\\anaconda\\envs\\faseeh\\lib\\site-packages (from wandb) (0.4.0)\n","Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in d:\\anaconda\\envs\\faseeh\\lib\\site-packages (from wandb) (3.1.43)\n","Requirement already satisfied: platformdirs in d:\\anaconda\\envs\\faseeh\\lib\\site-packages (from wandb) (4.2.2)\n","Requirement already satisfied: protobuf!=4.21.0,<6,>=3.19.0 in d:\\anaconda\\envs\\faseeh\\lib\\site-packages (from wandb) (5.27.3)\n","Requirement already satisfied: psutil>=5.0.0 in d:\\anaconda\\envs\\faseeh\\lib\\site-packages (from wandb) (5.9.0)\n","Requirement already satisfied: pyyaml in d:\\anaconda\\envs\\faseeh\\lib\\site-packages (from wandb) (6.0.2)\n","Requirement already satisfied: requests<3,>=2.0.0 in d:\\anaconda\\envs\\faseeh\\lib\\site-packages (from wandb) (2.32.3)\n","Requirement already satisfied: sentry-sdk>=1.0.0 in d:\\anaconda\\envs\\faseeh\\lib\\site-packages (from wandb) (2.13.0)\n","Requirement already satisfied: setproctitle in d:\\anaconda\\envs\\faseeh\\lib\\site-packages (from wandb) (1.3.3)\n","Requirement already satisfied: setuptools in d:\\anaconda\\envs\\faseeh\\lib\\site-packages (from wandb) (72.1.0)\n","Requirement already satisfied: dill in d:\\anaconda\\envs\\faseeh\\lib\\site-packages (from evaluate) (0.3.8)\n","Requirement already satisfied: pandas in d:\\anaconda\\envs\\faseeh\\lib\\site-packages (from evaluate) (2.2.2)\n","Requirement already satisfied: tqdm>=4.62.1 in d:\\anaconda\\envs\\faseeh\\lib\\site-packages (from evaluate) (4.66.5)\n","Requirement already satisfied: xxhash in d:\\anaconda\\envs\\faseeh\\lib\\site-packages (from evaluate) (3.4.1)\n","Requirement already satisfied: multiprocess in d:\\anaconda\\envs\\faseeh\\lib\\site-packages (from evaluate) (0.70.16)\n","Requirement already satisfied: fsspec>=2021.05.0 in d:\\anaconda\\envs\\faseeh\\lib\\site-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.2.0)\n","Requirement already satisfied: packaging in d:\\anaconda\\envs\\faseeh\\lib\\site-packages (from evaluate) (24.1)\n","Requirement already satisfied: filelock in d:\\anaconda\\envs\\faseeh\\lib\\site-packages (from huggingface_hub) (3.13.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in d:\\anaconda\\envs\\faseeh\\lib\\site-packages (from huggingface_hub) (4.12.2)\n","Requirement already satisfied: pyarrow>=15.0.0 in d:\\anaconda\\envs\\faseeh\\lib\\site-packages (from datasets) (17.0.0)\n","Requirement already satisfied: aiohttp in d:\\anaconda\\envs\\faseeh\\lib\\site-packages (from datasets) (3.10.3)\n","Requirement already satisfied: torch>=1.0.0 in d:\\anaconda\\envs\\faseeh\\lib\\site-packages (from bert_score) (2.4.0+cu118)\n","Requirement already satisfied: matplotlib in d:\\anaconda\\envs\\faseeh\\lib\\site-packages (from bert_score) (3.9.2)\n","Requirement already satisfied: regex!=2019.12.17 in d:\\anaconda\\envs\\faseeh\\lib\\site-packages (from transformers) (2024.7.24)\n","Requirement already satisfied: safetensors>=0.4.1 in d:\\anaconda\\envs\\faseeh\\lib\\site-packages (from transformers) (0.4.4)\n","Requirement already satisfied: tokenizers<0.20,>=0.19 in d:\\anaconda\\envs\\faseeh\\lib\\site-packages (from transformers) (0.19.1)\n","Requirement already satisfied: colorama in d:\\anaconda\\envs\\faseeh\\lib\\site-packages (from click!=8.0.0,>=7.1->wandb) (0.4.6)\n","Requirement already satisfied: six>=1.4.0 in d:\\anaconda\\envs\\faseeh\\lib\\site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in d:\\anaconda\\envs\\faseeh\\lib\\site-packages (from aiohttp->datasets) (2.3.5)\n","Requirement already satisfied: aiosignal>=1.1.2 in d:\\anaconda\\envs\\faseeh\\lib\\site-packages (from aiohttp->datasets) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in d:\\anaconda\\envs\\faseeh\\lib\\site-packages (from aiohttp->datasets) (24.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in d:\\anaconda\\envs\\faseeh\\lib\\site-packages (from aiohttp->datasets) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in d:\\anaconda\\envs\\faseeh\\lib\\site-packages (from aiohttp->datasets) (6.0.5)\n","Requirement already satisfied: yarl<2.0,>=1.0 in d:\\anaconda\\envs\\faseeh\\lib\\site-packages (from aiohttp->datasets) (1.9.4)\n","Requirement already satisfied: gitdb<5,>=4.0.1 in d:\\anaconda\\envs\\faseeh\\lib\\site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n","Requirement already satisfied: python-dateutil>=2.8.2 in d:\\anaconda\\envs\\faseeh\\lib\\site-packages (from pandas->evaluate) (2.9.0)\n","Requirement already satisfied: pytz>=2020.1 in d:\\anaconda\\envs\\faseeh\\lib\\site-packages (from pandas->evaluate) (2024.1)\n","Requirement already satisfied: tzdata>=2022.7 in d:\\anaconda\\envs\\faseeh\\lib\\site-packages (from pandas->evaluate) (2024.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in d:\\anaconda\\envs\\faseeh\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in d:\\anaconda\\envs\\faseeh\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\anaconda\\envs\\faseeh\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (2.2.2)\n","Requirement already satisfied: certifi>=2017.4.17 in d:\\anaconda\\envs\\faseeh\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (2024.7.4)\n","Requirement already satisfied: sympy in d:\\anaconda\\envs\\faseeh\\lib\\site-packages (from torch>=1.0.0->bert_score) (1.12)\n","Requirement already satisfied: networkx in d:\\anaconda\\envs\\faseeh\\lib\\site-packages (from torch>=1.0.0->bert_score) (3.2.1)\n","Requirement already satisfied: jinja2 in d:\\anaconda\\envs\\faseeh\\lib\\site-packages (from torch>=1.0.0->bert_score) (3.1.3)\n","Requirement already satisfied: contourpy>=1.0.1 in d:\\anaconda\\envs\\faseeh\\lib\\site-packages (from matplotlib->bert_score) (1.2.1)\n","Requirement already satisfied: cycler>=0.10 in d:\\anaconda\\envs\\faseeh\\lib\\site-packages (from matplotlib->bert_score) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in d:\\anaconda\\envs\\faseeh\\lib\\site-packages (from matplotlib->bert_score) (4.53.1)\n","Requirement already satisfied: kiwisolver>=1.3.1 in d:\\anaconda\\envs\\faseeh\\lib\\site-packages (from matplotlib->bert_score) (1.4.5)\n","Requirement already satisfied: pillow>=8 in d:\\anaconda\\envs\\faseeh\\lib\\site-packages (from matplotlib->bert_score) (10.2.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in d:\\anaconda\\envs\\faseeh\\lib\\site-packages (from matplotlib->bert_score) (3.1.2)\n","Requirement already satisfied: smmap<6,>=3.0.1 in d:\\anaconda\\envs\\faseeh\\lib\\site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n","Requirement already satisfied: MarkupSafe>=2.0 in d:\\anaconda\\envs\\faseeh\\lib\\site-packages (from jinja2->torch>=1.0.0->bert_score) (2.1.5)\n","Requirement already satisfied: mpmath>=0.19 in d:\\anaconda\\envs\\faseeh\\lib\\site-packages (from sympy->torch>=1.0.0->bert_score) (1.3.0)\n","Downloading wandb-0.17.7-py3-none-win_amd64.whl (6.5 MB)\n","   ---------------------------------------- 0.0/6.5 MB ? eta -:--:--\n","   - -------------------------------------- 0.3/6.5 MB ? eta -:--:--\n","   -------- ------------------------------- 1.3/6.5 MB 4.8 MB/s eta 0:00:02\n","   ---------------------- ----------------- 3.7/6.5 MB 8.1 MB/s eta 0:00:01\n","   ------------------------------------- -- 6.0/6.5 MB 9.2 MB/s eta 0:00:01\n","   ---------------------------------------- 6.5/6.5 MB 8.9 MB/s eta 0:00:00\n","Installing collected packages: wandb\n","  Attempting uninstall: wandb\n","    Found existing installation: wandb 0.17.6\n","    Uninstalling wandb-0.17.6:\n","      Successfully uninstalled wandb-0.17.6\n","Successfully installed wandb-0.17.7\n"]}],"source":["!pip install -U wandb evaluate huggingface_hub datasets bert_score evaluate transformers numpy==1.26"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-07-19T13:10:38.913130Z","iopub.status.busy":"2024-07-19T13:10:38.912355Z","iopub.status.idle":"2024-07-19T13:10:38.918655Z","shell.execute_reply":"2024-07-19T13:10:38.917695Z","shell.execute_reply.started":"2024-07-19T13:10:38.913097Z"},"trusted":true},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import torch\n","import os\n","from torch.utils.data import DataLoader, random_split, Dataset\n","from datasets import load_dataset\n","\n","# USE RAY TUNE. https://docs.ray.io/en/latest/train/examples/intel_gaudi/bert.html\n","# USE E5SCORE AS A LOSS\n","# deepl translations"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-07-19T13:10:47.097269Z","iopub.status.busy":"2024-07-19T13:10:47.096723Z","iopub.status.idle":"2024-07-19T13:10:47.565382Z","shell.execute_reply":"2024-07-19T13:10:47.564486Z","shell.execute_reply.started":"2024-07-19T13:10:47.097240Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mabdulmohsena\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: C:\\Users\\user\\.netrc\n"]}],"source":["import wandb\n","from huggingface_hub import HfApi, HfFolder\n","from transformers import set_seed\n","\n","try: # If it is on Kaggle\n","    from kaggle_secrets import UserSecretsClient\n","    user_secrets = UserSecretsClient()\n","\n","    HF_TOKEN = user_secrets.get_secret(\"HF_TOKEN\")\n","    WANDB_KEY = user_secrets.get_secret(\"WANDB_KEY\")\n","\n","except ModuleNotFoundError: # If it is local\n","    HF_TOKEN = os.environ[\"HF_TOKEN\"]\n","    WANDB_KEY = os.environ[\"WANDB_KEY\"]\n","    \n","\n","HfFolder.save_token(HF_TOKEN)\n","wandb.login(key=WANDB_KEY)\n","\n","# Reproducibility\n","seed = 1\n","torch.cuda.manual_seed_all(seed)\n","torch.backends.cudnn.deterministic = True\n","torch.backends.cudnn.benchmark = False\n","set_seed(seed)\n","np.random.seed(seed)"]},{"cell_type":"markdown","metadata":{},"source":["## Modeling"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-07-19T13:32:58.713938Z","iopub.status.busy":"2024-07-19T13:32:58.713156Z","iopub.status.idle":"2024-07-19T13:33:00.113072Z","shell.execute_reply":"2024-07-19T13:33:00.112278Z","shell.execute_reply.started":"2024-07-19T13:32:58.713903Z"},"trusted":true},"outputs":[],"source":["from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, GenerationConfig\n","from transformers import DataCollatorForSeq2Seq"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-07-15T16:27:12.085792Z","iopub.status.busy":"2024-07-15T16:27:12.085447Z","iopub.status.idle":"2024-07-15T16:27:12.090538Z","shell.execute_reply":"2024-07-15T16:27:12.089577Z","shell.execute_reply.started":"2024-07-15T16:27:12.085762Z"},"trusted":true},"outputs":[],"source":["# # Configure any model from HF HUB\n","# assert input(\"YOU WILL REMOVE THE HUB MODEL FOR THIS, TYPE 'OK' TO PROCEED: \").upper() == 'OK'\n","# model_name = \"facebook/mbart-large-50-many-to-many-mmt\"\n","# model_name = \"facebook/m2m100_1.2B\"\n","# #model_name= \"Helsinki-NLP/opus-mt-en-ar\"\n","# model_name= \"facebook/nllb-200-distilled-600M\"\n","\n","# tokenizer = AutoTokenizer.from_pretrained(model_name)\n","# model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n","# generation_config = GenerationConfig(\n","#     max_length=200,\n","#     forced_bos_token_id=256011, # Arabic\n","#     num_beams = 4,\n","#     early_stopping=True,\n","#     do_sample=True,\n","#     top_k=50,\n","    \n","#     # Testing Config\n","# #     num_return_sequences=4, # Number of sentences to generate\n","# #     return_dict_in_generate=True, # Returns the complete generation data from within the model.\n","# #     output_scores=True, # Score of each token.\n","# )\n","\n","# tokenizer.src_lang=\"eng_Latn\"\n","# tokenizer.tgt_lang=\"arb_Arab\"\n","\n","# model.push_to_hub(\"Abdulmohsena/Faseeh\")\n","# tokenizer.push_to_hub(\"Abdulmohsena/Faseeh\")"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-07-19T13:35:16.261115Z","iopub.status.busy":"2024-07-19T13:35:16.260373Z","iopub.status.idle":"2024-07-19T13:36:13.818020Z","shell.execute_reply":"2024-07-19T13:36:13.817097Z","shell.execute_reply.started":"2024-07-19T13:35:16.261085Z"},"trusted":true},"outputs":[],"source":["# Instantiating The Model\n","model_name = \"Abdulmohsena/Faseeh\"\n","\n","tokenizer = AutoTokenizer.from_pretrained(model_name, src_lang=\"eng_Latn\", tgt_lang=\"arb_Arab\")\n","model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n","generation_config = GenerationConfig.from_pretrained(model_name)\n","\n","# https://huggingface.co/docs/transformers/en/main_classes/text_generation"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-07-19T13:37:01.272040Z","iopub.status.busy":"2024-07-19T13:37:01.271384Z","iopub.status.idle":"2024-07-19T13:37:03.237270Z","shell.execute_reply":"2024-07-19T13:37:03.236371Z","shell.execute_reply.started":"2024-07-19T13:37:01.272009Z"},"trusted":true},"outputs":[{"data":{"text/plain":["'وآمن وزير خارجية السعودية الزوار أن الأمن دائم الأولوية القصوى.'"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["# Sanity Check\n","dummy = \"And the Saudi Arabian Foreign Minister assured the visitors that security is always a top priority.\"\n","\n","encoded_ar = tokenizer(dummy, return_tensors=\"pt\")\n","generated_tokens = model.generate(**encoded_ar, generation_config=generation_config)\n","\n","tokenizer.decode(generated_tokens[0], skip_special_tokens=True)"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["dataset = load_dataset(\"Abdulmohsena/Classic-Arabic-English-Language-Pairs\")\n","\n","dataset = dataset['train']"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-07-19T13:37:10.633447Z","iopub.status.busy":"2024-07-19T13:37:10.632511Z","iopub.status.idle":"2024-07-19T13:37:30.702252Z","shell.execute_reply":"2024-07-19T13:37:30.701489Z","shell.execute_reply.started":"2024-07-19T13:37:10.633412Z"},"trusted":true},"outputs":[],"source":["preprocess_function = lambda examples: tokenizer(\n","        examples['source'], text_target=examples['target'], max_length=128, truncation=True, padding=True)\n","\n","tokenized_dataset = dataset.map(preprocess_function, batched=True)\n","tokenized_dataset = tokenized_dataset.train_test_split(test_size=0.25)"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-07-19T13:37:32.739829Z","iopub.status.busy":"2024-07-19T13:37:32.738929Z","iopub.status.idle":"2024-07-19T13:37:32.744046Z","shell.execute_reply":"2024-07-19T13:37:32.743116Z","shell.execute_reply.started":"2024-07-19T13:37:32.739795Z"},"trusted":true},"outputs":[],"source":["data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model_name)\n","\n","# from transformers import DataCollatorForLanguageModeling # NOT GOOD FOR SEMANTIC TRANSLATION\n","\n","# data_collator = DataCollatorForLanguageModeling(\n","#     tokenizer=tokenizer,\n","#     mlm=True,             # Whether to use Masked Language Modeling (MLM)\n","#     mlm_probability=0.15  # Probability of masking tokens for MLM\n","# )"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-07-19T13:37:50.231712Z","iopub.status.busy":"2024-07-19T13:37:50.231077Z","iopub.status.idle":"2024-07-19T13:37:51.760121Z","shell.execute_reply":"2024-07-19T13:37:51.759178Z","shell.execute_reply.started":"2024-07-19T13:37:50.231679Z"},"trusted":true},"outputs":[],"source":["import numpy as np\n","import evaluate\n","import transformers\n","\n","metric = evaluate.load(\"bertscore\")\n","\n","def postprocess_text(preds, labels):\n","    preds = [pred.strip() for pred in preds]\n","    labels = [label.strip() for label in labels]\n","\n","    return preds, labels\n","\n","def compute_metrics(eval_preds): \n","    \n","    preds, labels = eval_preds\n","    \n","    # Replace unknown labels\n","    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n","    \n","    # Decode tokens into text\n","    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n","    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n","\n","    # Postprocess text for cleaniness\n","    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n","\n","    # Get Average bertscore F-1\n","    result = metric.compute(predictions=decoded_preds, references=decoded_labels, lang=\"ar\")\n","    result = {\"bertscore-f1\": np.mean(result['f1'])}\n","\n","    # Get avg gen length\n","    prediction_lengths = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n","    result[\"gen_len\"] = np.mean(prediction_lengths)\n","\n","    result = {k: round(v, 4) for k, v in result.items()} # Round to 4 figures\n","\n","    return result"]},{"cell_type":"markdown","metadata":{},"source":["## Training"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-07-15T17:18:30.493318Z","iopub.status.busy":"2024-07-15T17:18:30.493049Z","iopub.status.idle":"2024-07-15T17:18:31.515562Z","shell.execute_reply":"2024-07-15T17:18:31.514819Z","shell.execute_reply.started":"2024-07-15T17:18:30.493295Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["d:\\anaconda\\envs\\Faseeh\\Lib\\site-packages\\accelerate\\accelerator.py:488: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n","  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"]}],"source":["from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer\n","train_batch_size = 2\n","torch.cuda.empty_cache()\n","\n","# https://huggingface.co/docs/transformers/v4.42.0/performance\n","training_args = Seq2SeqTrainingArguments(\n","    output_dir=f\"Faseeh_{metric.name}\",\n","    save_total_limit=1,\n","    \n","    per_device_train_batch_size=train_batch_size,\n","    per_device_eval_batch_size=train_batch_size,\n","    gradient_accumulation_steps=4 // train_batch_size,\n","    gradient_checkpointing=True,\n","    # torch_compile=True,\n","    \n","    logging_strategy=\"steps\",\n","    logging_steps=500, \n","    \n","    eval_strategy='epoch',\n","    \n","    weight_decay=0.01,\n","    warmup_steps=1_000,\n","    learning_rate=3e-5,\n","    lr_scheduler_type=\"cosine\",\n","    \n","    \n","    num_train_epochs=1,\n","    \n","    predict_with_generate=True,\n","    fp16=True,\n","    push_to_hub=True,\n","    report_to='wandb'\n",")\n","\n","trainer = Seq2SeqTrainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=tokenized_dataset[\"train\"],\n","    eval_dataset=tokenized_dataset[\"test\"],\n","    tokenizer=tokenizer,\n","    data_collator=data_collator,\n","    compute_metrics=compute_metrics,\n",")"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-07-15T17:19:43.302545Z","iopub.status.busy":"2024-07-15T17:19:43.301899Z","iopub.status.idle":"2024-07-15T22:08:17.678766Z","shell.execute_reply":"2024-07-15T22:08:17.678014Z","shell.execute_reply.started":"2024-07-15T17:19:43.302508Z"},"trusted":true},"outputs":[{"data":{"text/html":["Tracking run with wandb version 0.17.7"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>c:\\Users\\user\\Desktop\\Prog\\Faseeh\\wandb\\run-20240816_133206-4hyyn2kg</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href='https://wandb.ai/abdulmohsena/Faseeh/runs/4hyyn2kg' target=\"_blank\">Run @ 2024-08-16 13:32:06.978180</a></strong> to <a href='https://wandb.ai/abdulmohsena/Faseeh' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View project at <a href='https://wandb.ai/abdulmohsena/Faseeh' target=\"_blank\">https://wandb.ai/abdulmohsena/Faseeh</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run at <a href='https://wandb.ai/abdulmohsena/Faseeh/runs/4hyyn2kg' target=\"_blank\">https://wandb.ai/abdulmohsena/Faseeh/runs/4hyyn2kg</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n","  0%|          | 0/15544 [00:00<?, ?it/s]d:\\anaconda\\envs\\Faseeh\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n","  return fn(*args, **kwargs)\n","`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n","d:\\anaconda\\envs\\Faseeh\\Lib\\site-packages\\torch\\utils\\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n","  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n","  1%|▏         | 232/15544 [44:27<50:29:47, 11.87s/it]"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[1;32mIn[14], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatetime\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m datetime\n\u001b[0;32m      3\u001b[0m wandb\u001b[38;5;241m.\u001b[39minit(project\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFaseeh\u001b[39m\u001b[38;5;124m\"\u001b[39m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRun @ \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdatetime\u001b[38;5;241m.\u001b[39mnow()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m wandb\u001b[38;5;241m.\u001b[39mfinish()\n","File \u001b[1;32md:\\anaconda\\envs\\Faseeh\\Lib\\site-packages\\transformers\\trainer.py:1939\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1936\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1937\u001b[0m     \u001b[38;5;66;03m# Disable progress bars when uploading models during checkpoints to avoid polluting stdout\u001b[39;00m\n\u001b[0;32m   1938\u001b[0m     hf_hub_utils\u001b[38;5;241m.\u001b[39mdisable_progress_bars()\n\u001b[1;32m-> 1939\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1940\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1941\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1942\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1943\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1944\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1945\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m   1946\u001b[0m     hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n","File \u001b[1;32md:\\anaconda\\envs\\Faseeh\\Lib\\site-packages\\transformers\\trainer.py:2294\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2288\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m   2289\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs)\n\u001b[0;32m   2291\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   2292\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   2293\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m-> 2294\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misinf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss_step\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   2295\u001b[0m ):\n\u001b[0;32m   2296\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   2297\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n\u001b[0;32m   2298\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n","\u001b[1;31mKeyboardInterrupt\u001b[0m: "]}],"source":["from datetime import datetime\n","\n","wandb.init(project=\"Faseeh\", name=f\"Run @ {datetime.now()}\")\n","trainer.train()\n","wandb.finish()"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[],"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.9"}},"nbformat":4,"nbformat_minor":4}
