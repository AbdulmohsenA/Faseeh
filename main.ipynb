{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-19T13:03:21.687670Z","iopub.status.busy":"2024-07-19T13:03:21.687393Z","iopub.status.idle":"2024-07-19T13:03:36.362734Z","shell.execute_reply":"2024-07-19T13:03:36.361792Z","shell.execute_reply.started":"2024-07-19T13:03:21.687646Z"},"trusted":true},"outputs":[],"source":["!pip install wandb evaluate huggingface_hub datasets bert_score evaluate"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-07-19T13:10:38.913130Z","iopub.status.busy":"2024-07-19T13:10:38.912355Z","iopub.status.idle":"2024-07-19T13:10:38.918655Z","shell.execute_reply":"2024-07-19T13:10:38.917695Z","shell.execute_reply.started":"2024-07-19T13:10:38.913097Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["\n","A module that was compiled using NumPy 1.x cannot be run in\n","NumPy 2.0.1 as it may crash. To support both 1.x and 2.x\n","versions of NumPy, modules must be compiled with NumPy 2.0.\n","Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n","\n","If you are a user of the module, the easiest solution will be to\n","downgrade to 'numpy<2' or try to upgrade the affected module.\n","We expect that some modules will need time to support NumPy 2.\n","\n","Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n","  File \"<frozen runpy>\", line 88, in _run_code\n","  File \"d:\\anaconda\\envs\\Faseeh\\Lib\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n","    app.launch_new_instance()\n","  File \"d:\\anaconda\\envs\\Faseeh\\Lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n","    app.start()\n","  File \"d:\\anaconda\\envs\\Faseeh\\Lib\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n","    self.io_loop.start()\n","  File \"d:\\anaconda\\envs\\Faseeh\\Lib\\site-packages\\tornado\\platform\\asyncio.py\", line 215, in start\n","    self.asyncio_loop.run_forever()\n","  File \"d:\\anaconda\\envs\\Faseeh\\Lib\\asyncio\\base_events.py\", line 608, in run_forever\n","    self._run_once()\n","  File \"d:\\anaconda\\envs\\Faseeh\\Lib\\asyncio\\base_events.py\", line 1936, in _run_once\n","    handle._run()\n","  File \"d:\\anaconda\\envs\\Faseeh\\Lib\\asyncio\\events.py\", line 84, in _run\n","    self._context.run(self._callback, *self._args)\n","  File \"d:\\anaconda\\envs\\Faseeh\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 545, in dispatch_queue\n","    await self.process_one()\n","  File \"d:\\anaconda\\envs\\Faseeh\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in process_one\n","    await dispatch(*args)\n","  File \"d:\\anaconda\\envs\\Faseeh\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 437, in dispatch_shell\n","    await result\n","  File \"d:\\anaconda\\envs\\Faseeh\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 362, in execute_request\n","    await super().execute_request(stream, ident, parent)\n","  File \"d:\\anaconda\\envs\\Faseeh\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 778, in execute_request\n","    reply_content = await reply_content\n","  File \"d:\\anaconda\\envs\\Faseeh\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 449, in do_execute\n","    res = shell.run_cell(\n","  File \"d:\\anaconda\\envs\\Faseeh\\Lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n","    return super().run_cell(*args, **kwargs)\n","  File \"d:\\anaconda\\envs\\Faseeh\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3075, in run_cell\n","    result = self._run_cell(\n","  File \"d:\\anaconda\\envs\\Faseeh\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3130, in _run_cell\n","    result = runner(coro)\n","  File \"d:\\anaconda\\envs\\Faseeh\\Lib\\site-packages\\IPython\\core\\async_helpers.py\", line 128, in _pseudo_sync_runner\n","    coro.send(None)\n","  File \"d:\\anaconda\\envs\\Faseeh\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3334, in run_cell_async\n","    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n","  File \"d:\\anaconda\\envs\\Faseeh\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3517, in run_ast_nodes\n","    if await self.run_code(code, result, async_=asy):\n","  File \"d:\\anaconda\\envs\\Faseeh\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3577, in run_code\n","    exec(code_obj, self.user_global_ns, self.user_ns)\n","  File \"C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_19428\\2450898130.py\", line 3, in <module>\n","    import torch\n","  File \"d:\\anaconda\\envs\\Faseeh\\Lib\\site-packages\\torch\\__init__.py\", line 2120, in <module>\n","    from torch._higher_order_ops import cond\n","  File \"d:\\anaconda\\envs\\Faseeh\\Lib\\site-packages\\torch\\_higher_order_ops\\__init__.py\", line 1, in <module>\n","    from .cond import cond\n","  File \"d:\\anaconda\\envs\\Faseeh\\Lib\\site-packages\\torch\\_higher_order_ops\\cond.py\", line 5, in <module>\n","    import torch._subclasses.functional_tensor\n","  File \"d:\\anaconda\\envs\\Faseeh\\Lib\\site-packages\\torch\\_subclasses\\functional_tensor.py\", line 42, in <module>\n","    class FunctionalTensor(torch.Tensor):\n","  File \"d:\\anaconda\\envs\\Faseeh\\Lib\\site-packages\\torch\\_subclasses\\functional_tensor.py\", line 258, in FunctionalTensor\n","    cpu = _conversion_method_template(device=torch.device(\"cpu\"))\n","d:\\anaconda\\envs\\Faseeh\\Lib\\site-packages\\torch\\_subclasses\\functional_tensor.py:258: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_numpy.cpp:84.)\n","  cpu = _conversion_method_template(device=torch.device(\"cpu\"))\n","d:\\anaconda\\envs\\Faseeh\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]}],"source":["import numpy as np\n","import pandas as pd\n","import torch\n","import os\n","from torch.utils.data import DataLoader, random_split, Dataset\n","from datasets import load_dataset\n","\n","# USE RAY TUNE. https://docs.ray.io/en/latest/train/examples/intel_gaudi/bert.html\n","# USE E5SCORE AS A LOSS\n","# deepl translations"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-07-19T13:10:47.097269Z","iopub.status.busy":"2024-07-19T13:10:47.096723Z","iopub.status.idle":"2024-07-19T13:10:47.565382Z","shell.execute_reply":"2024-07-19T13:10:47.564486Z","shell.execute_reply.started":"2024-07-19T13:10:47.097240Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mabdulmohsena\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: C:\\Users\\user\\.netrc\n"]}],"source":["import wandb\n","from huggingface_hub import HfApi, HfFolder\n","from transformers import set_seed\n","\n","try: # If it is on Kaggle\n","    from kaggle_secrets import UserSecretsClient\n","    user_secrets = UserSecretsClient()\n","\n","    HF_TOKEN = user_secrets.get_secret(\"HF_TOKEN\")\n","    WANDB_KEY = user_secrets.get_secret(\"WANDB_KEY\")\n","\n","except ModuleNotFoundError: # If it is local\n","    HF_TOKEN = os.environ[\"HF_TOKEN\"]\n","    WANDB_KEY = os.environ[\"WANDB_KEY\"]\n","    \n","\n","HfFolder.save_token(HF_TOKEN)\n","wandb.login(key=WANDB_KEY)\n","\n","# Reproducibility\n","seed = 1\n","torch.cuda.manual_seed_all(seed)\n","torch.backends.cudnn.deterministic = True\n","torch.backends.cudnn.benchmark = False\n","set_seed(seed)\n","np.random.seed(seed)"]},{"cell_type":"markdown","metadata":{},"source":["## Modeling"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-07-19T13:32:58.713938Z","iopub.status.busy":"2024-07-19T13:32:58.713156Z","iopub.status.idle":"2024-07-19T13:33:00.113072Z","shell.execute_reply":"2024-07-19T13:33:00.112278Z","shell.execute_reply.started":"2024-07-19T13:32:58.713903Z"},"trusted":true},"outputs":[],"source":["from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, GenerationConfig\n","from transformers import DataCollatorForSeq2Seq"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-07-15T16:27:12.085792Z","iopub.status.busy":"2024-07-15T16:27:12.085447Z","iopub.status.idle":"2024-07-15T16:27:12.090538Z","shell.execute_reply":"2024-07-15T16:27:12.089577Z","shell.execute_reply.started":"2024-07-15T16:27:12.085762Z"},"trusted":true},"outputs":[],"source":["# # Configure any model from HF HUB\n","# assert input(\"YOU WILL REMOVE THE HUB MODEL FOR THIS, TYPE 'OK' TO PROCEED: \").upper() == 'OK'\n","# model_name = \"facebook/mbart-large-50-many-to-many-mmt\"\n","# model_name = \"facebook/m2m100_1.2B\"\n","# #model_name= \"Helsinki-NLP/opus-mt-en-ar\"\n","# model_name= \"facebook/nllb-200-distilled-600M\"\n","\n","# tokenizer = AutoTokenizer.from_pretrained(model_name)\n","# model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n","# generation_config = GenerationConfig(\n","#     max_length=200,\n","#     forced_bos_token_id=256011, # Arabic\n","#     num_beams = 4,\n","#     early_stopping=True,\n","#     do_sample=True,\n","#     top_k=50,\n","    \n","#     # Testing Config\n","# #     num_return_sequences=4, # Number of sentences to generate\n","# #     return_dict_in_generate=True, # Returns the complete generation data from within the model.\n","# #     output_scores=True, # Score of each token.\n","# )\n","\n","# tokenizer.src_lang=\"eng_Latn\"\n","# tokenizer.tgt_lang=\"arb_Arab\"\n","\n","# model.push_to_hub(\"Abdulmohsena/Faseeh\")\n","# tokenizer.push_to_hub(\"Abdulmohsena/Faseeh\")"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-07-19T13:35:16.261115Z","iopub.status.busy":"2024-07-19T13:35:16.260373Z","iopub.status.idle":"2024-07-19T13:36:13.818020Z","shell.execute_reply":"2024-07-19T13:36:13.817097Z","shell.execute_reply.started":"2024-07-19T13:35:16.261085Z"},"trusted":true},"outputs":[],"source":["# Instantiating The Model\n","model_name = \"Abdulmohsena/Faseeh\"\n","\n","tokenizer = AutoTokenizer.from_pretrained(model_name, src_lang=\"eng_Latn\", tgt_lang=\"arb_Arab\")\n","model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n","generation_config = GenerationConfig.from_pretrained(model_name)\n","\n","# https://huggingface.co/docs/transformers/en/main_classes/text_generation"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-07-19T13:37:01.272040Z","iopub.status.busy":"2024-07-19T13:37:01.271384Z","iopub.status.idle":"2024-07-19T13:37:03.237270Z","shell.execute_reply":"2024-07-19T13:37:03.236371Z","shell.execute_reply.started":"2024-07-19T13:37:01.272009Z"},"trusted":true},"outputs":[{"data":{"text/plain":["'وآمن وزير خارجية السعودية الزوار أن الأمن دائم الأولوية القصوى.'"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["# Sanity Check\n","dummy = \"And the Saudi Arabian Foreign Minister assured the visitors that security is always a top priority.\"\n","\n","encoded_ar = tokenizer(dummy, return_tensors=\"pt\")\n","generated_tokens = model.generate(**encoded_ar, generation_config=generation_config)\n","\n","tokenizer.decode(generated_tokens[0], skip_special_tokens=True)"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["dataset = load_dataset(\"Abdulmohsena/Classic-Arabic-English-Language-Pairs\")\n","\n","dataset = dataset['train']"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-07-19T13:37:10.633447Z","iopub.status.busy":"2024-07-19T13:37:10.632511Z","iopub.status.idle":"2024-07-19T13:37:30.702252Z","shell.execute_reply":"2024-07-19T13:37:30.701489Z","shell.execute_reply.started":"2024-07-19T13:37:10.633412Z"},"trusted":true},"outputs":[],"source":["preprocess_function = lambda examples: tokenizer(\n","        examples['source'], text_target=examples['target'], max_length=128, truncation=True, padding=True)\n","\n","tokenized_dataset = dataset.map(preprocess_function, batched=True)\n","tokenized_dataset = tokenized_dataset.train_test_split(test_size=0.25)"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-07-19T13:37:32.739829Z","iopub.status.busy":"2024-07-19T13:37:32.738929Z","iopub.status.idle":"2024-07-19T13:37:32.744046Z","shell.execute_reply":"2024-07-19T13:37:32.743116Z","shell.execute_reply.started":"2024-07-19T13:37:32.739795Z"},"trusted":true},"outputs":[],"source":["data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model_name)\n","\n","# from transformers import DataCollatorForLanguageModeling # NOT GOOD FOR SEMANTIC TRANSLATION\n","\n","# data_collator = DataCollatorForLanguageModeling(\n","#     tokenizer=tokenizer,\n","#     mlm=True,             # Whether to use Masked Language Modeling (MLM)\n","#     mlm_probability=0.15  # Probability of masking tokens for MLM\n","# )"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2024-07-19T13:37:50.231712Z","iopub.status.busy":"2024-07-19T13:37:50.231077Z","iopub.status.idle":"2024-07-19T13:37:51.760121Z","shell.execute_reply":"2024-07-19T13:37:51.759178Z","shell.execute_reply.started":"2024-07-19T13:37:50.231679Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Downloading builder script: 100%|██████████| 7.95k/7.95k [00:00<?, ?B/s]\n"]}],"source":["import numpy as np\n","import evaluate\n","import transformers\n","\n","metric = evaluate.load(\"bertscore\")\n","\n","def postprocess_text(preds, labels):\n","    preds = [pred.strip() for pred in preds]\n","    labels = [label.strip() for label in labels]\n","\n","    return preds, labels\n","\n","def compute_metrics(eval_preds): \n","    \n","    preds, labels = eval_preds\n","    \n","    # Replace unknown labels\n","    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n","    \n","    # Decode tokens into text\n","    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n","    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n","\n","    # Postprocess text for cleaniness\n","    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n","\n","    # Get Average bertscore F-1\n","    result = metric.compute(predictions=decoded_preds, references=decoded_labels, lang=\"ar\")\n","    result = {\"bertscore-f1\": np.mean(result['f1'])}\n","\n","    # Get avg gen length\n","    prediction_lengths = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n","    result[\"gen_len\"] = np.mean(prediction_lengths)\n","\n","    result = {k: round(v, 4) for k, v in result.items()} # Round to 4 figures\n","\n","    return result"]},{"cell_type":"markdown","metadata":{},"source":["## Training"]},{"cell_type":"code","execution_count":85,"metadata":{"execution":{"iopub.execute_input":"2024-07-19T13:40:07.136058Z","iopub.status.busy":"2024-07-19T13:40:07.135691Z","iopub.status.idle":"2024-07-19T13:40:23.898901Z","shell.execute_reply":"2024-07-19T13:40:23.898013Z","shell.execute_reply.started":"2024-07-19T13:40:07.136030Z"},"trusted":true},"outputs":[{"data":{"text/html":["Tracking run with wandb version 0.17.4"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/kaggle/working/wandb/run-20240719_134007-yv6eemhw</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href='https://wandb.ai/abdulmohsena/Faseeh/runs/yv6eemhw' target=\"_blank\">Run @ 2024-07-19 13:40:07.137285</a></strong> to <a href='https://wandb.ai/abdulmohsena/Faseeh' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View project at <a href='https://wandb.ai/abdulmohsena/Faseeh' target=\"_blank\">https://wandb.ai/abdulmohsena/Faseeh</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run at <a href='https://wandb.ai/abdulmohsena/Faseeh/runs/yv6eemhw' target=\"_blank\">https://wandb.ai/abdulmohsena/Faseeh/runs/yv6eemhw</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/abdulmohsena/Faseeh/runs/yv6eemhw?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"],"text/plain":["<wandb.sdk.wandb_run.Run at 0x7bc2c6c6e0b0>"]},"execution_count":85,"metadata":{},"output_type":"execute_result"}],"source":["from datetime import datetime\n","wandb.init(project=\"Faseeh\", name=f\"Run @ {datetime.now()}\")"]},{"cell_type":"code","execution_count":24,"metadata":{"execution":{"iopub.execute_input":"2024-07-15T17:18:30.493318Z","iopub.status.busy":"2024-07-15T17:18:30.493049Z","iopub.status.idle":"2024-07-15T17:18:31.515562Z","shell.execute_reply":"2024-07-15T17:18:31.514819Z","shell.execute_reply.started":"2024-07-15T17:18:30.493295Z"},"trusted":true},"outputs":[{"ename":"ImportError","evalue":"Using the `Trainer` with `PyTorch` requires `accelerate>=0.21.0`: Please run `pip install transformers[torch]` or `pip install accelerate -U`","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)","Cell \u001b[1;32mIn[24], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# https://huggingface.co/docs/transformers/v4.42.0/performance\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m training_args \u001b[38;5;241m=\u001b[39m \u001b[43mSeq2SeqTrainingArguments\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mFaseeh_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mmetric\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_total_limit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mper_device_train_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mper_device_eval_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgradient_accumulation_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgradient_checkpointing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# torch_compile=True,\u001b[39;49;00m\n\u001b[0;32m     15\u001b[0m \u001b[43m    \u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogging_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msteps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogging_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m    \u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mepoch\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m    \u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwarmup_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1_000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3e-5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr_scheduler_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcosine\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[43m    \u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[43m    \u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_train_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[43m    \u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpredict_with_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfp16\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpush_to_hub\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreport_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mwandb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[0;32m     33\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     35\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Seq2SeqTrainer(\n\u001b[0;32m     36\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m     37\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     42\u001b[0m     compute_metrics\u001b[38;5;241m=\u001b[39mcompute_metrics,\n\u001b[0;32m     43\u001b[0m )\n","File \u001b[1;32m<string>:136\u001b[0m, in \u001b[0;36m__init__\u001b[1;34m(self, output_dir, overwrite_output_dir, do_train, do_eval, do_predict, eval_strategy, prediction_loss_only, per_device_train_batch_size, per_device_eval_batch_size, per_gpu_train_batch_size, per_gpu_eval_batch_size, gradient_accumulation_steps, eval_accumulation_steps, eval_delay, torch_empty_cache_steps, learning_rate, weight_decay, adam_beta1, adam_beta2, adam_epsilon, max_grad_norm, num_train_epochs, max_steps, lr_scheduler_type, lr_scheduler_kwargs, warmup_ratio, warmup_steps, log_level, log_level_replica, log_on_each_node, logging_dir, logging_strategy, logging_first_step, logging_steps, logging_nan_inf_filter, save_strategy, save_steps, save_total_limit, save_safetensors, save_on_each_node, save_only_model, restore_callback_states_from_checkpoint, no_cuda, use_cpu, use_mps_device, seed, data_seed, jit_mode_eval, use_ipex, bf16, fp16, fp16_opt_level, half_precision_backend, bf16_full_eval, fp16_full_eval, tf32, local_rank, ddp_backend, tpu_num_cores, tpu_metrics_debug, debug, dataloader_drop_last, eval_steps, dataloader_num_workers, dataloader_prefetch_factor, past_index, run_name, disable_tqdm, remove_unused_columns, label_names, load_best_model_at_end, metric_for_best_model, greater_is_better, ignore_data_skip, fsdp, fsdp_min_num_params, fsdp_config, fsdp_transformer_layer_cls_to_wrap, accelerator_config, deepspeed, label_smoothing_factor, optim, optim_args, adafactor, group_by_length, length_column_name, report_to, ddp_find_unused_parameters, ddp_bucket_cap_mb, ddp_broadcast_buffers, dataloader_pin_memory, dataloader_persistent_workers, skip_memory_metrics, use_legacy_prediction_loop, push_to_hub, resume_from_checkpoint, hub_model_id, hub_strategy, hub_token, hub_private_repo, hub_always_push, gradient_checkpointing, gradient_checkpointing_kwargs, include_inputs_for_metrics, eval_do_concat_batches, fp16_backend, evaluation_strategy, push_to_hub_model_id, push_to_hub_organization, push_to_hub_token, mp_parameters, auto_find_batch_size, full_determinism, torchdynamo, ray_scope, ddp_timeout, torch_compile, torch_compile_backend, torch_compile_mode, dispatch_batches, split_batches, include_tokens_per_second, include_num_input_tokens_seen, neftune_noise_alpha, optim_target_modules, batch_eval_metrics, eval_on_start, eval_use_gather_object, sortish_sampler, predict_with_generate, generation_max_length, generation_num_beams, generation_config)\u001b[0m\n","File \u001b[1;32md:\\anaconda\\envs\\Faseeh\\Lib\\site-packages\\transformers\\training_args.py:1730\u001b[0m, in \u001b[0;36mTrainingArguments.__post_init__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1728\u001b[0m \u001b[38;5;66;03m# Initialize device before we proceed\u001b[39;00m\n\u001b[0;32m   1729\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_torch_available():\n\u001b[1;32m-> 1730\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\n\u001b[0;32m   1732\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtorchdynamo \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1733\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1734\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`torchdynamo` is deprecated and will be removed in version 5 of 🤗 Transformers. Use\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1735\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `torch_compile_backend` instead\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1736\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m   1737\u001b[0m     )\n","File \u001b[1;32md:\\anaconda\\envs\\Faseeh\\Lib\\site-packages\\transformers\\training_args.py:2227\u001b[0m, in \u001b[0;36mTrainingArguments.device\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2223\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2224\u001b[0m \u001b[38;5;124;03mThe device used by this process.\u001b[39;00m\n\u001b[0;32m   2225\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2226\u001b[0m requires_backends(\u001b[38;5;28mself\u001b[39m, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m-> 2227\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setup_devices\u001b[49m\n","File \u001b[1;32md:\\anaconda\\envs\\Faseeh\\Lib\\site-packages\\transformers\\utils\\generic.py:60\u001b[0m, in \u001b[0;36mcached_property.__get__\u001b[1;34m(self, obj, objtype)\u001b[0m\n\u001b[0;32m     58\u001b[0m cached \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(obj, attr, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cached \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 60\u001b[0m     cached \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;28msetattr\u001b[39m(obj, attr, cached)\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cached\n","File \u001b[1;32md:\\anaconda\\envs\\Faseeh\\Lib\\site-packages\\transformers\\training_args.py:2103\u001b[0m, in \u001b[0;36mTrainingArguments._setup_devices\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2101\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_sagemaker_mp_enabled():\n\u001b[0;32m   2102\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_accelerate_available():\n\u001b[1;32m-> 2103\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[0;32m   2104\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing the `Trainer` with `PyTorch` requires `accelerate>=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mACCELERATE_MIN_VERSION\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2105\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease run `pip install transformers[torch]` or `pip install accelerate -U`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2106\u001b[0m         )\n\u001b[0;32m   2107\u001b[0m \u001b[38;5;66;03m# We delay the init of `PartialState` to the end for clarity\u001b[39;00m\n\u001b[0;32m   2108\u001b[0m accelerator_state_kwargs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menabled\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_configured_state\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m}\n","\u001b[1;31mImportError\u001b[0m: Using the `Trainer` with `PyTorch` requires `accelerate>=0.21.0`: Please run `pip install transformers[torch]` or `pip install accelerate -U`"]}],"source":["from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer\n","\n","torch.cuda.empty_cache()\n","\n","# https://huggingface.co/docs/transformers/v4.42.0/performance\n","training_args = Seq2SeqTrainingArguments(\n","    output_dir=f\"Faseeh_{metric.name}\",\n","    save_total_limit=1,\n","    \n","    per_device_train_batch_size=4,\n","    per_device_eval_batch_size=4,\n","    gradient_accumulation_steps=4,\n","    gradient_checkpointing=True,\n","    # torch_compile=True,\n","    \n","    logging_strategy=\"steps\",\n","    logging_steps=500, \n","    \n","    eval_strategy='epoch',\n","    \n","    weight_decay=0.01,\n","    warmup_steps=1_000,\n","    learning_rate=3e-5,\n","    lr_scheduler_type=\"cosine\",\n","    \n","    \n","    num_train_epochs=4,\n","    \n","    predict_with_generate=True,\n","    fp16=True,\n","    push_to_hub=True,\n","    report_to='wandb'\n",")\n","\n","trainer = Seq2SeqTrainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=tokenized_dataset[\"train\"],\n","    eval_dataset=tokenized_dataset[\"test\"],\n","    tokenizer=tokenizer,\n","    data_collator=data_collator,\n","    compute_metrics=compute_metrics,\n",")"]},{"cell_type":"code","execution_count":90,"metadata":{"execution":{"iopub.execute_input":"2024-07-15T17:19:43.302545Z","iopub.status.busy":"2024-07-15T17:19:43.301899Z","iopub.status.idle":"2024-07-15T22:08:17.678766Z","shell.execute_reply":"2024-07-15T22:08:17.678014Z","shell.execute_reply.started":"2024-07-15T17:19:43.302508Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='3886' max='3886' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [3886/3886 4:48:26, Epoch 0/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Bertscore-f1</th>\n","      <th>Gen Len</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>0</td>\n","      <td>0.015200</td>\n","      <td>0.058165</td>\n","      <td>0.984500</td>\n","      <td>37.320100</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n","Non-default generation parameters: {'max_length': 200}\n","Your generation config was originally created from the model config, but the model config has changed since then. Unless you pass the `generation_config` argument to this model's `generate` calls, they will revert to the legacy behavior where the base `generate` parameterization is loaded from the model config instead. To avoid this behavior and this warning, we recommend you to overwrite the generation config model attribute before calling the model's `save_pretrained`, preferably also removing any generation kwargs from the model config. This warning will be raised to an exception in v4.41.\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n","Non-default generation parameters: {'max_length': 200}\n","Your generation config was originally created from the model config, but the model config has changed since then. Unless you pass the `generation_config` argument to this model's `generate` calls, they will revert to the legacy behavior where the base `generate` parameterization is loaded from the model config instead. To avoid this behavior and this warning, we recommend you to overwrite the generation config model attribute before calling the model's `save_pretrained`, preferably also removing any generation kwargs from the model config. This warning will be raised to an exception in v4.41.\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n","Non-default generation parameters: {'max_length': 200}\n","Your generation config was originally created from the model config, but the model config has changed since then. Unless you pass the `generation_config` argument to this model's `generate` calls, they will revert to the legacy behavior where the base `generate` parameterization is loaded from the model config instead. To avoid this behavior and this warning, we recommend you to overwrite the generation config model attribute before calling the model's `save_pretrained`, preferably also removing any generation kwargs from the model config. This warning will be raised to an exception in v4.41.\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n","Non-default generation parameters: {'max_length': 200}\n","Your generation config was originally created from the model config, but the model config has changed since then. Unless you pass the `generation_config` argument to this model's `generate` calls, they will revert to the legacy behavior where the base `generate` parameterization is loaded from the model config instead. To avoid this behavior and this warning, we recommend you to overwrite the generation config model attribute before calling the model's `save_pretrained`, preferably also removing any generation kwargs from the model config. This warning will be raised to an exception in v4.41.\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n","Non-default generation parameters: {'max_length': 200}\n","Your generation config was originally created from the model config, but the model config has changed since then. Unless you pass the `generation_config` argument to this model's `generate` calls, they will revert to the legacy behavior where the base `generate` parameterization is loaded from the model config instead. To avoid this behavior and this warning, we recommend you to overwrite the generation config model attribute before calling the model's `save_pretrained`, preferably also removing any generation kwargs from the model config. This warning will be raised to an exception in v4.41.\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n","Non-default generation parameters: {'max_length': 200}\n","Your generation config was originally created from the model config, but the model config has changed since then. Unless you pass the `generation_config` argument to this model's `generate` calls, they will revert to the legacy behavior where the base `generate` parameterization is loaded from the model config instead. To avoid this behavior and this warning, we recommend you to overwrite the generation config model attribute before calling the model's `save_pretrained`, preferably also removing any generation kwargs from the model config. This warning will be raised to an exception in v4.41.\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n","Non-default generation parameters: {'max_length': 200}\n","Your generation config was originally created from the model config, but the model config has changed since then. Unless you pass the `generation_config` argument to this model's `generate` calls, they will revert to the legacy behavior where the base `generate` parameterization is loaded from the model config instead. To avoid this behavior and this warning, we recommend you to overwrite the generation config model attribute before calling the model's `save_pretrained`, preferably also removing any generation kwargs from the model config. This warning will be raised to an exception in v4.41.\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n","Non-default generation parameters: {'max_length': 200}\n","Your generation config was originally created from the model config, but the model config has changed since then. Unless you pass the `generation_config` argument to this model's `generate` calls, they will revert to the legacy behavior where the base `generate` parameterization is loaded from the model config instead. To avoid this behavior and this warning, we recommend you to overwrite the generation config model attribute before calling the model's `save_pretrained`, preferably also removing any generation kwargs from the model config. This warning will be raised to an exception in v4.41.\n","/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1364: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n","  warnings.warn(\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["VBox(children=(Label(value='0.004 MB of 0.004 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<style>\n","    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n","    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n","    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n","    </style>\n","<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/bertscore-f1</td><td>▁</td></tr><tr><td>eval/gen_len</td><td>▁</td></tr><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁▂▃▄▅▆▇██</td></tr><tr><td>train/global_step</td><td>▁▂▃▄▅▆▇██</td></tr><tr><td>train/grad_norm</td><td>█▁▃▅▃▃▃</td></tr><tr><td>train/learning_rate</td><td>▄█▇▆▄▂▁</td></tr><tr><td>train/loss</td><td>█▃▁▁▁▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/bertscore-f1</td><td>0.9845</td></tr><tr><td>eval/gen_len</td><td>37.3201</td></tr><tr><td>eval/loss</td><td>0.05817</td></tr><tr><td>eval/runtime</td><td>9230.0527</td></tr><tr><td>eval/samples_per_second</td><td>2.245</td></tr><tr><td>eval/steps_per_second</td><td>0.561</td></tr><tr><td>total_flos</td><td>1.6842736967614464e+16</td></tr><tr><td>train/epoch</td><td>0.99994</td></tr><tr><td>train/global_step</td><td>3886</td></tr><tr><td>train/grad_norm</td><td>0.10105</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.0152</td></tr><tr><td>train_loss</td><td>0.01839</td></tr><tr><td>train_runtime</td><td>17308.9254</td></tr><tr><td>train_samples_per_second</td><td>3.592</td></tr><tr><td>train_steps_per_second</td><td>0.225</td></tr></table><br/></div></div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run <strong style=\"color:#cdcd00\">unique-oath-52</strong> at: <a href='https://wandb.ai/abdulmohsena/Faseeh/runs/yf5k1vbr' target=\"_blank\">https://wandb.ai/abdulmohsena/Faseeh/runs/yf5k1vbr</a><br/> View project at: <a href='https://wandb.ai/abdulmohsena/Faseeh' target=\"_blank\">https://wandb.ai/abdulmohsena/Faseeh</a><br/>Synced 5 W&B file(s), 0 media file(s), 1 artifact file(s) and 0 other file(s)"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Find logs at: <code>./wandb/run-20240715_171809-yf5k1vbr/logs</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}],"source":["trainer.train()\n","wandb.finish()"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[],"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.9"}},"nbformat":4,"nbformat_minor":4}
