{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import torch\n",
    "from accelerate import Accelerator\n",
    "import os\n",
    "import numpy as np\n",
    "from huggingface_hub import HfApi, HfFolder\n",
    "import transformers\n",
    "from tqdm import tqdm\n",
    "from peft import PeftModel, prepare_model_for_kbit_training\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, GenerationConfig\n",
    "from transformers import DataCollatorForSeq2Seq, BitsAndBytesConfig\n",
    "from datasets import load_dataset\n",
    "from evaluate import load\n",
    "from transformers import get_scheduler\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mabdulmohsena\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: C:\\Users\\user\\.netrc\n"
     ]
    }
   ],
   "source": [
    "try: # If it is on Kaggle\n",
    "    from kaggle_secrets import UserSecretsClient\n",
    "    user_secrets = UserSecretsClient()\n",
    "\n",
    "    HF_TOKEN = user_secrets.get_secret(\"HF_TOKEN\")\n",
    "    WANDB_KEY = user_secrets.get_secret(\"WANDB_KEY\")\n",
    "\n",
    "except ModuleNotFoundError: # If it is local\n",
    "    HF_TOKEN = os.environ[\"HF_TOKEN\"]\n",
    "    WANDB_KEY = os.environ[\"WANDB_KEY\"]\n",
    "    \n",
    "\n",
    "HfFolder.save_token(HF_TOKEN)\n",
    "wandb.login(key=WANDB_KEY)\n",
    "\n",
    "# Reproducibility\n",
    "seed = 1\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "transformers.set_seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 4,718,592 || all params: 619,792,384 || trainable%: 0.7613\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model_name = \"AbdulmohsenA/Faseeh\"\n",
    "lora_name = \"AbdulmohsenA/Faseeh_LoRA\"\n",
    "\n",
    "quantization_config=BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\"\n",
    "    )\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name,\n",
    "                                                quantization_config=quantization_config,\n",
    "                                                torch_dtype=torch.float16,\n",
    "                                                low_cpu_mem_usage=True)\n",
    "prepare_model_for_kbit_training(model)\n",
    "\n",
    "model = PeftModel.from_pretrained(model, lora_name, is_trainable=True)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"Abdulmohsena/Faseeh\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, src_lang=\"eng_Latn\", tgt_lang=\"arb_Arab\")\n",
    "generation_config = GenerationConfig.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"Abdulmohsena/Classic-Arabic-English-Language-Pairs\")\n",
    "\n",
    "dataset = dataset['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_function = lambda examples: tokenizer(\n",
    "        examples['source'], text_target=examples['target'], max_length=256, truncation=True, padding=True, return_tensors='pt')\n",
    "\n",
    "tokenized_dataset = dataset.map(preprocess_function, batched=True, remove_columns=['source', 'target'])\n",
    "tokenized_dataset = tokenized_dataset.train_test_split(test_size=0.20)\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model, padding=True, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess_text(preds, labels):\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [label.strip() for label in labels]\n",
    "    return preds, labels\n",
    "\n",
    "metric = load(\"bertscore\")\n",
    "\n",
    "def compute_metrics(preds, labels):\n",
    "    labels = torch.where(labels != -100, labels, torch.tensor(tokenizer.pad_token_id).to(labels.device))\n",
    "\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "    # Postprocess text to remove unnecessary spaces\n",
    "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
    "\n",
    "    bertscore_results = metric.compute(\n",
    "        predictions=decoded_preds, \n",
    "        references=decoded_labels, \n",
    "        lang=\"ar\",\n",
    "        device='cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    )\n",
    "    \n",
    "    # Get the average generation length\n",
    "    prediction_lengths = [(pred != tokenizer.pad_token_id).sum().item() for pred in preds]\n",
    "    \n",
    "    # Prepare final result\n",
    "    result = {\n",
    "        \"precision\": round(np.mean(bertscore_results['precision']), 4),\n",
    "        \"recall\": round(np.mean(bertscore_results['recall']), 4),\n",
    "        \"f1\": round(np.mean(bertscore_results['f1']), 4),\n",
    "        \"gen_len\": round(np.mean(prediction_lengths), 4)\n",
    "    }\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "AcceleratorState has already been initialized and cannot be changed, restart your runtime completely and pass `mixed_precision='fp16'` to `Accelerator()`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m accelerator \u001b[38;5;241m=\u001b[39m \u001b[43mAccelerator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmixed_precision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfp16\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mto(accelerator\u001b[38;5;241m.\u001b[39mdevice)  \u001b[38;5;66;03m# Send the model to device\u001b[39;00m\n\u001b[0;32m      4\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdamW(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5e-5\u001b[39m, weight_decay\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m)\n",
      "File \u001b[1;32md:\\anaconda\\envs\\Faseeh\\Lib\\site-packages\\accelerate\\accelerator.py:384\u001b[0m, in \u001b[0;36mAccelerator.__init__\u001b[1;34m(self, device_placement, split_batches, mixed_precision, gradient_accumulation_steps, cpu, dataloader_config, deepspeed_plugin, fsdp_plugin, megatron_lm_plugin, rng_types, log_with, project_dir, project_config, gradient_accumulation_plugin, dispatch_batches, even_batches, use_seedable_sampler, step_scheduler_with_optimizer, kwargs_handlers, dynamo_backend)\u001b[0m\n\u001b[0;32m    381\u001b[0m                 \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprofile_handler \u001b[38;5;241m=\u001b[39m handler\n\u001b[0;32m    383\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minit_handler\u001b[38;5;241m.\u001b[39mto_kwargs() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minit_handler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[1;32m--> 384\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m=\u001b[39m \u001b[43mAcceleratorState\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    385\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmixed_precision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmixed_precision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    386\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcpu\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    387\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdynamo_plugin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdynamo_plugin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    388\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdeepspeed_plugin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdeepspeed_plugin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    389\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfsdp_plugin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfsdp_plugin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    390\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmegatron_lm_plugin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmegatron_lm_plugin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    391\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_from_accelerator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    392\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    393\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    395\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mmixed_precision \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfp8\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp8_recipe_handler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    396\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp8_recipe_handler \u001b[38;5;241m=\u001b[39m FP8RecipeKwargs()\n",
      "File \u001b[1;32md:\\anaconda\\envs\\Faseeh\\Lib\\site-packages\\accelerate\\state.py:862\u001b[0m, in \u001b[0;36mAcceleratorState.__init__\u001b[1;34m(self, mixed_precision, cpu, dynamo_plugin, deepspeed_plugin, fsdp_plugin, megatron_lm_plugin, _from_accelerator, **kwargs)\u001b[0m\n\u001b[0;32m    860\u001b[0m     PartialState(cpu, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    861\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m\u001b[38;5;241m.\u001b[39mupdate(PartialState\u001b[38;5;241m.\u001b[39m_shared_state)\n\u001b[1;32m--> 862\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_initialized\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmixed_precision\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcpu\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    863\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minitialized:\n\u001b[0;32m    864\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeepspeed_plugin \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\anaconda\\envs\\Faseeh\\Lib\\site-packages\\accelerate\\state.py:963\u001b[0m, in \u001b[0;36mAcceleratorState._check_initialized\u001b[1;34m(self, mixed_precision, cpu)\u001b[0m\n\u001b[0;32m    957\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(err\u001b[38;5;241m.\u001b[39mformat(flag\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu=True\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m    958\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    959\u001b[0m     mixed_precision \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    960\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m mixed_precision \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mixed_precision\n\u001b[0;32m    961\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m!=\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED\n\u001b[0;32m    962\u001b[0m ):\n\u001b[1;32m--> 963\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(err\u001b[38;5;241m.\u001b[39mformat(flag\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmixed_precision=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmixed_precision\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "\u001b[1;31mValueError\u001b[0m: AcceleratorState has already been initialized and cannot be changed, restart your runtime completely and pass `mixed_precision='fp16'` to `Accelerator()`."
     ]
    }
   ],
   "source": [
    "accelerator = Accelerator(mixed_precision='fp16')\n",
    "\n",
    "model = model.to(accelerator.device)  # Send the model to device\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5, weight_decay=0.01)\n",
    "\n",
    "# Define learning rate scheduler\n",
    "num_update_steps_per_epoch = len(tokenized_dataset[\"train\"]) // (24)\n",
    "num_training_steps = num_update_steps_per_epoch * 2\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=\"cosine\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=50,\n",
    "    num_training_steps=num_training_steps\n",
    ")\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(tokenized_dataset[\"train\"], batch_size=24, shuffle=True, collate_fn=data_collator)\n",
    "eval_dataloader = torch.utils.data.DataLoader(tokenized_dataset[\"test\"], batch_size=2, collate_fn=data_collator)\n",
    "\n",
    "model, optimizer, train_dataloader, eval_dataloader, lr_scheduler = accelerator.prepare(\n",
    "    model, optimizer, train_dataloader, eval_dataloader, lr_scheduler\n",
    ")\n",
    "\n",
    "model.gradient_checkpointing_disable()\n",
    "model.gradient_checkpointing_kwargs = {\"use_reentrant\": False}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_dataloader, optimizer, lr_scheduler, accelerator):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    progress_bar = tqdm(train_dataloader)\n",
    "\n",
    "    for step, batch in enumerate(progress_bar):\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        accelerator.backward(loss)\n",
    "        \n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "\n",
    "        progress_bar.set_postfix({\"loss\": train_loss / (step + 1)})\n",
    "\n",
    "    return train_loss / len(train_dataloader)\n",
    "\n",
    "def evaluate_model(model, eval_dataloader, accelerator):\n",
    "    model.eval()\n",
    "\n",
    "    eval_f1 = 0\n",
    "    eval_loss = 0\n",
    "    progress_bar = tqdm(eval_dataloader)\n",
    "    \n",
    "    for step, batch in enumerate(progress_bar):\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "\n",
    "            # Convert logits to tokens\n",
    "            predictions = torch.argmax(outputs.logits, dim=2)\n",
    "            labels = batch['labels']\n",
    "\n",
    "            predictions = accelerator.gather(predictions)\n",
    "            labels = accelerator.gather(labels)\n",
    "\n",
    "            metrics = compute_metrics(predictions, labels)\n",
    "            \n",
    "        eval_loss += outputs.loss.item()\n",
    "        eval_f1 += metrics['f1']\n",
    "\n",
    "        progress_bar.set_postfix({\"loss\": eval_loss / (step + 1)})\n",
    "\n",
    "    return {\"eval_loss\": eval_loss / len(eval_dataloader),\n",
    "                     \"f1\": eval_f1 / len(eval_dataloader)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(tokenized_dataset[\"train\"].select(range(5)), batch_size=1, shuffle=True, collate_fn=data_collator)\n",
    "eval_dataloader = torch.utils.data.DataLoader(tokenized_dataset[\"test\"].select(range(5)), batch_size=2, collate_fn=data_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>d:\\Prog\\Faseeh\\train\\wandb\\run-20241006_182216-2yu7n5yv</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/abdulmohsena/Faseeh/runs/2yu7n5yv' target=\"_blank\">Faseeh @ 2024-10-06 18:22:16.920861</a></strong> to <a href='https://wandb.ai/abdulmohsena/Faseeh' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/abdulmohsena/Faseeh' target=\"_blank\">https://wandb.ai/abdulmohsena/Faseeh</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/abdulmohsena/Faseeh/runs/2yu7n5yv' target=\"_blank\">https://wandb.ai/abdulmohsena/Faseeh/runs/2yu7n5yv</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:03<00:00,  1.56it/s, loss=0.0481]\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]d:\\anaconda\\envs\\Faseeh\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\Faseeh\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:439: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      "100%|██████████| 3/3 [00:08<00:00,  2.73s/it, loss=0.0263]\n",
      "100%|██████████| 5/5 [00:02<00:00,  1.76it/s, loss=0.0383]\n",
      "100%|██████████| 3/3 [00:04<00:00,  1.43s/it, loss=0.0262]\n"
     ]
    }
   ],
   "source": [
    "wandb.init(project=\"Faseeh\", name=F\"Faseeh @ {datetime.now()}\")\n",
    "torch.cuda.empty_cache()\n",
    "for epoch in range(2):\n",
    "    train_loss = train_model(model, train_dataloader, optimizer, lr_scheduler, accelerator)\n",
    "    eval_metrics = evaluate_model(model, eval_dataloader, accelerator)\n",
    "    \n",
    "    eval_metrics['train_loss'] = train_loss\n",
    "    wandb.log(eval_metrics, step=epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Removed shared tensor {'base_model.model.model.encoder.embed_tokens.weight', 'base_model.model.model.decoder.embed_tokens.weight', 'base_model.model.lm_head.weight'} while saving. This should be OK, but check by verifying that you don't receive any warning while reloading\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "555b93d0c93a458e9196544524ccf809",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.007 MB of 0.007 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval_loss</td><td>█▁</td></tr><tr><td>f1</td><td>▁▁</td></tr><tr><td>train_loss</td><td>█▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval_loss</td><td>0.02616</td></tr><tr><td>f1</td><td>0.97957</td></tr><tr><td>train_loss</td><td>0.03833</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Faseeh @ 2024-10-06 18:22:16.920861</strong> at: <a href='https://wandb.ai/abdulmohsena/Faseeh/runs/2yu7n5yv' target=\"_blank\">https://wandb.ai/abdulmohsena/Faseeh/runs/2yu7n5yv</a><br/> View project at: <a href='https://wandb.ai/abdulmohsena/Faseeh' target=\"_blank\">https://wandb.ai/abdulmohsena/Faseeh</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20241006_182216-2yu7n5yv\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = accelerator.unwrap_model(model)\n",
    "# Save model artifact\n",
    "model_artifact = wandb.Artifact(\"model\", type=\"model\")\n",
    "\n",
    "accelerator.save_model(model, \"../experiments/models/\")\n",
    "model_artifact.add_file(\"../experiments/models/model.safetensors\")\n",
    "\n",
    "# wandb.log_artifact(model_artifact)\n",
    "model.push_to_hub(\"Abdulmohsena/Faseeh_LoRA\", token=True, max_shard_size=\"5GB\", safe_serialization=True)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bbc2bec0f9f4eba9a7bd798f2dcc5be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/5.17k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\Faseeh\\Lib\\site-packages\\huggingface_hub\\file_download.py:147: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\user\\.cache\\huggingface\\hub\\models--AbdulmohsenA--Faseeh_LoRA. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39db9b9cfef44c8db8373bca467f2cd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/18.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/Abdulmohsena/Faseeh_LoRA/commit/edc22b26c2743456afc9c947f43c7e03460caa45', commit_message='Upload model', commit_description='', oid='edc22b26c2743456afc9c947f43c7e03460caa45', pr_url=None, repo_url=RepoUrl('https://huggingface.co/Abdulmohsena/Faseeh_LoRA', endpoint='https://huggingface.co', repo_type='model', repo_id='Abdulmohsena/Faseeh_LoRA'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
